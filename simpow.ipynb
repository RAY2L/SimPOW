{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFsvfhxjfeoE",
        "outputId": "890c09ea-8330-4b97-a4cd-11cd93ac6f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: activate: No such file or directory\n",
            "Cloning into 'alignment-handbook'...\n",
            "remote: Enumerating objects: 1136, done.\u001b[K\n",
            "remote: Counting objects: 100% (736/736), done.\u001b[K\n",
            "remote: Compressing objects: 100% (305/305), done.\u001b[K\n",
            "remote: Total 1136 (delta 535), reused 507 (delta 403), pack-reused 400\u001b[K\n",
            "Receiving objects: 100% (1136/1136), 260.40 KiB | 1.61 MiB/s, done.\n",
            "Resolving deltas: 100% (636/636), done.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Processing /home/ubuntu/ray/alignment-handbook\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting accelerate>=0.29.2 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting bitsandbytes>=0.43.0 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting datasets>=2.18.0 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting deepspeed==0.12.2 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading deepspeed-0.12.2.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting einops>=0.6.1 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting evaluate==0.4.0 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting hf_transfer>=0.1.4 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.19.2 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: jinja2>=3.0.0 in /usr/lib/python3/dist-packages (from alignment-handbook==0.4.0.dev0) (3.0.3)\n",
            "Collecting ninja>=1.11.1 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0) (1.25.2)\n",
            "Collecting packaging>=23.0 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting peft>=0.9.0 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting protobuf<=3.20.2 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Collecting safetensors>=0.3.3 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from alignment-handbook==0.4.0.dev0) (1.8.0)\n",
            "Collecting sentencepiece>=0.1.99 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/lib/python3/dist-packages (from alignment-handbook==0.4.0.dev0) (2.13.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0) (4.66.1)\n",
            "Collecting transformers>=4.39.3 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.8.2 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting hjson (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0) (5.9.0)\n",
            "Collecting py-cpuinfo (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Requirement already satisfied: pydantic in /home/ubuntu/.local/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0) (2.5.2)\n",
            "Collecting pynvml (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0) (2.0.1)\n",
            "Collecting dill (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0) (2.31.0)\n",
            "Collecting xxhash (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting responses<0.19 (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.29.2->alignment-handbook==0.4.0.dev0) (5.4.1)\n",
            "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0) (3.6.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting requests>=2.19.0 (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tqdm>=4.64.1 (from alignment-handbook==0.4.0.dev0)\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.2->alignment-handbook==0.4.0.dev0) (4.8.0)\n",
            "Collecting regex!=2019.12.17 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.8.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0) (1.26.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0) (2020.6.20)\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0) (2.14.5)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: alignment-handbook, deepspeed\n",
            "  Building wheel for alignment-handbook (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for alignment-handbook: filename=alignment_handbook-0.4.0.dev0-py3-none-any.whl size=22201 sha256=1ffc35c28ca546d7dec4d91535569d5c62ad806eda611059f6e6c6f781725117\n",
            "  Stored in directory: /home/ubuntu/.cache/pip/wheels/db/19/4a/e9be6d30ec4b39c58fa9558a2b7e7ec6b9eae5ff6f07bdaf84\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265690 sha256=9c60eeaa01639a3e3777850ce846a32663f65f7ef635de6e3f7959e944658d30\n",
            "  Stored in directory: /home/ubuntu/.cache/pip/wheels/06/c8/39/10f68166de0a2a12c511c3c0569128b62be534edc9a224782c\n",
            "Successfully built alignment-handbook deepspeed\n",
            "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: sentencepiece, py-cpuinfo, ninja, hjson, xxhash, tqdm, shtab, safetensors, requests, regex, pynvml, pygments, pyarrow-hotfix, pyarrow, protobuf, packaging, multidict, mdurl, hf_transfer, fsspec, frozenlist, einops, docstring-parser, dill, bitsandbytes, async-timeout, yarl, responses, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, deepspeed, aiohttp, accelerate, tyro, transformers, peft, datasets, trl, evaluate, alignment-handbook\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "Successfully installed accelerate-0.31.0 aiohttp-3.9.5 aiosignal-1.3.1 alignment-handbook-0.4.0.dev0 async-timeout-4.0.3 bitsandbytes-0.43.1 datasets-2.20.0 deepspeed-0.12.2 dill-0.3.8 docstring-parser-0.16 einops-0.8.0 evaluate-0.4.0 frozenlist-1.4.1 fsspec-2024.5.0 hf_transfer-0.1.6 hjson-3.1.0 huggingface-hub-0.23.3 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.5 multiprocess-0.70.16 ninja-1.11.1.1 packaging-24.1 peft-0.11.1 protobuf-3.20.2 py-cpuinfo-9.0.0 pyarrow-16.1.0 pyarrow-hotfix-0.6 pygments-2.18.0 pynvml-11.5.0 regex-2024.5.15 requests-2.32.3 responses-0.18.0 rich-13.7.1 safetensors-0.4.3 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.41.2 trl-0.9.4 tyro-0.8.4 xxhash-3.4.1 yarl-1.9.4\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install Conda\n",
        "# !pip install -q condacolab\n",
        "# import condacolab\n",
        "# condacolab.install()\n",
        "\n",
        "# Create a virtual environment and install dependencies\n",
        "!conda create -n handbook python=3.10 -y\n",
        "!source activate handbook\n",
        "\n",
        "# Clone the alignment-handbook repository and install requirements\n",
        "!git clone https://github.com/huggingface/alignment-handbook.git\n",
        "!cd alignment-handbook && python -m pip install .\n",
        "\n",
        "# Install Flash Attention 2\n",
        "# !python -m pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NCg4GlZ5lxrK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.3.1\n",
            "Uninstalling torch-2.3.1:\n",
            "  Successfully uninstalled torch-2.3.1\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting torch==2.2.2\n",
            "  Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (1.12.1)\n",
            "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (2024.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.2.2) (12.1.105)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jinja2->torch==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: triton, nvidia-nccl-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "Successfully installed nvidia-nccl-cu12-2.19.3 torch-2.2.2 triton-2.2.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Remove existing versions of torch\n",
        "!pip uninstall -y torch\n",
        "\n",
        "# Install PyTorch 2.2.2\n",
        "!pip install torch==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "6d1abc28ed00413cba61edffa52ca51b",
            "4d8fa997a6bd4f70a85bbd1fcc24a14c",
            "3d31744072ca479792f8e6fc15697bac",
            "db58c927b0564b4eba2084c8c6419cf0",
            "b46ce49263864ea69abfc64ea490554b",
            "c60b117cdbc34017b5a87d99ab2b5b0c",
            "5634a0ba7df043b28414f2485143c150",
            "c4fab0e2c93a47919a4f5f6faa69f34c",
            "758f405ed9844b4fa0a700ea23d4ac56",
            "8eb2c633455440018ebf35f1cb28f785",
            "eedb241c467d438a829193e1d11c3c4e",
            "811234b35e424e1fbfb413edd8071ce2",
            "af57d4292200416b9f35a1819ebb99e1",
            "1709c0dae6754fb7b5b3d36ad8656e60",
            "43b3553fc00a4090a8eeae07abc21300",
            "30c0bc0798564e2fb336f08efc49f8d7",
            "2ab95b523a0f4472af91e13f72b3c368"
          ]
        },
        "id": "IioHPPR5kUfY",
        "outputId": "a560980e-c80d-493b-c0a3-5ca319493a84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "809a2d4112f048eabbad10981ab40909",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "# 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the Hugging Face token environment variable\n",
        "os.environ['HF_TOKEN'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/.local/bin/huggingface-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 51, in main\n",
            "    service.run()\n",
            "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
            "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
            "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 115, in login\n",
            "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
            "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 191, in interpreter_login\n",
            "    token = getpass(\"Enter your token (input will not be visible): \")\n",
            "  File \"/usr/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
            "    passwd = _raw_input(prompt, stream, input=input)\n",
            "  File \"/usr/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
            "    line = input.readline()\n",
            "  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n",
            "    def decode(self, input, final=False):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tb6xB5FrEfy",
        "outputId": "c53f5660-87ff-4aec-cfa5-649e000fb1a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting flash_attn\n",
            "  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting einops (from flash_attn)\n",
            "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting torch (from flash_attn)\n",
            "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting filelock (from torch->flash_attn)\n",
            "  Downloading filelock-3.15.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch->flash_attn)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch->flash_attn)\n",
            "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch->flash_attn)\n",
            "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting jinja2 (from torch->flash_attn)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch->flash_attn)\n",
            "  Using cached fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->flash_attn)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->flash_attn)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->flash_attn)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->flash_attn)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->flash_attn)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->flash_attn)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->flash_attn)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->flash_attn)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->flash_attn)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->flash_attn)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->flash_attn)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.1 (from torch->flash_attn)\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->flash_attn)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->flash_attn)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch->flash_attn)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:03\u001b[0mm\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading filelock-3.15.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "Building wheels for collected packages: flash_attn\n",
            "  Building wheel for flash_attn (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for flash_attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=120821333 sha256=7bfd5ecaaf20577cd1255eaa90d9008a09050b3408ba6388bcbc5b6144f482d0\n",
            "  Stored in directory: /home/ubuntu/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\n",
            "Successfully built flash_attn\n",
            "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, einops, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, flash_attn\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.8.0\n",
            "    Uninstalling typing_extensions-4.8.0:\n",
            "      Successfully uninstalled typing_extensions-4.8.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.40\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.40:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.40\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.5.0\n",
            "    Uninstalling fsspec-2024.5.0:\n",
            "      Successfully uninstalled fsspec-2024.5.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.0\n",
            "    Uninstalling einops-0.8.0:\n",
            "      Successfully uninstalled einops-0.8.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.2\n",
            "    Uninstalling torch-2.2.2:\n",
            "      Successfully uninstalled torch-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.20.0 requires fsspec[http]<=2024.5.0,>=2023.1.0, but you have fsspec 2024.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 einops-0.8.0 filelock-3.15.1 flash_attn-2.5.9.post1 fsspec-2024.6.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sympy-1.12.1 torch-2.3.1 triton-2.3.1 typing-extensions-4.12.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install flash_attn -U --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-z-rPkm3Vh7",
        "outputId": "9970f4a4-a8b9-4f5a-8613-93d3e5f84029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.1-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/lib/python3/dist-packages (from wandb) (8.0.3)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/lib/python3/dist-packages (from wandb) (2.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb) (3.20.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/lib/python3/dist-packages (from wandb) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from wandb) (5.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (59.6.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2020.6.20)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.0.0->wandb)\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.1-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: urllib3, smmap, setproctitle, docker-pycreds, sentry-sdk, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.5.1 setproctitle-1.3.3 smmap-5.0.1 urllib3-2.2.1 wandb-0.17.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XJB4gaX3YdV",
        "outputId": "ce478be1-df64-49b1-f358-7b33da328bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraylee0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login\n",
        "# 581f87f7486be24c8d3dbbb61132408d5af92280"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['WANDB_API_KEY'] = '581f87f7486be24c8d3dbbb61132408d5af92280'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6QfKcSw4sObu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'SimPOW/.git/objects/pack': Directory not empty\n",
            "rm: cannot remove 'SimPOW/wandb/run-20240613_222205-0zf4uwm0/logs': Directory not empty\n"
          ]
        }
      ],
      "source": [
        "!rm -rf SimPOW/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euRwRdWsgobW",
        "outputId": "06b0ea59-e51d-45ac-f08f-3d2ad4887b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'SimPOW_2'...\n",
            "remote: Enumerating objects: 260, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 260 (delta 11), reused 21 (delta 6), pack-reused 224\u001b[K\n",
            "Receiving objects: 100% (260/260), 996.31 KiB | 7.27 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/RAY2L/SimPOW.git SimPOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzbSAW6A36uH",
        "outputId": "3576e713-f4df-427d-fa0f-aa3309ee318d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65536+0 records in\n",
            "65536+0 records out\n",
            "68719476736 bytes (69 GB, 64 GiB) copied, 199.353 s, 345 MB/s\n",
            "Setting up swapspace version 1, size = 64 GiB (68719472640 bytes)\n",
            "no label, UUID=1a404a97-c3a5-4a1a-838c-7fc6e24010d3\n",
            "swapon: /mnt/data/swapfile: swapon failed: Invalid argument\n"
          ]
        }
      ],
      "source": [
        "!sudo mkdir -p /mnt/data\n",
        "!sudo dd if=/dev/zero of=/mnt/data/swapfile bs=1M count=65536\n",
        "!sudo chmod 600 /mnt/data/swapfile\n",
        "!sudo mkswap /mnt/data/swapfile\n",
        "!sudo swapon /mnt/data/swapfile\n",
        "!sudo swapon --show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.1.0+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (2200.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.15.2+cu121 (from versions: 0.1.6, 0.2.0, 0.12.0, 0.12.0+cpu, 0.12.0+cu102, 0.12.0+cu113, 0.12.0+cu115, 0.12.0+rocm4.3.1, 0.12.0+rocm4.5.2, 0.13.0, 0.13.0+cpu, 0.13.0+cu102, 0.13.0+cu113, 0.13.0+cu116, 0.13.0+rocm5.0, 0.13.0+rocm5.1.1, 0.13.1, 0.13.1+cpu, 0.13.1+cu102, 0.13.1+cu113, 0.13.1+cu116, 0.13.1+rocm5.0, 0.13.1+rocm5.1.1, 0.14.0, 0.14.0+cpu, 0.14.0+cu116, 0.14.0+cu117, 0.14.0+rocm5.1.1, 0.14.0+rocm5.2, 0.14.1, 0.14.1+cpu, 0.14.1+cu116, 0.14.1+cu117, 0.14.1+rocm5.1.1, 0.14.1+rocm5.2, 0.15.0+cpu, 0.15.0+cu117, 0.15.0+cu118, 0.15.0+rocm5.3, 0.15.0+rocm5.4.2, 0.15.1, 0.15.1+cpu, 0.15.1+cu117, 0.15.1+cu118, 0.15.1+rocm5.3, 0.15.1+rocm5.4.2, 0.15.2, 0.15.2+cpu, 0.15.2+cu117, 0.15.2+cu118, 0.15.2+rocm5.3, 0.15.2+rocm5.4.2, 0.16.0, 0.16.0+cpu, 0.16.0+cu118, 0.16.0+cu121, 0.16.0+rocm5.5, 0.16.0+rocm5.6, 0.16.1, 0.16.1+cpu, 0.16.1+cu118, 0.16.1+cu121, 0.16.1+rocm5.5, 0.16.1+rocm5.6, 0.16.2, 0.16.2+cpu, 0.16.2+cu118, 0.16.2+cu121, 0.16.2+rocm5.5, 0.16.2+rocm5.6, 0.17.0, 0.17.0+cpu, 0.17.0+cu118, 0.17.0+cu121, 0.17.0+rocm5.6, 0.17.0+rocm5.7, 0.17.1, 0.17.1+cpu, 0.17.1+cu118, 0.17.1+cu121, 0.17.1+rocm5.6, 0.17.1+rocm5.7, 0.17.2, 0.17.2+cpu, 0.17.2+cu118, 0.17.2+cu121, 0.17.2+rocm5.6, 0.17.2+rocm5.7, 0.18.0, 0.18.0+cpu, 0.18.0+cu118, 0.18.0+cu121, 0.18.0+rocm5.7, 0.18.0+rocm6.0, 0.18.1, 0.18.1+cpu, 0.18.1+cu118, 0.18.1+cu121, 0.18.1+rocm5.7, 0.18.1+rocm6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.15.2+cu121\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.1.0+cu121 torchvision==0.15.2+cu121 torchaudio==2.1.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: flash-attn 2.5.9.post1\n",
            "Uninstalling flash-attn-2.5.9.post1:\n",
            "  Successfully uninstalled flash-attn-2.5.9.post1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall flash_attn -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting flash_attn\n",
            "  Using cached flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (from flash_attn) (2.3.1)\n",
            "Requirement already satisfied: einops in /home/ubuntu/.local/lib/python3.10/site-packages (from flash_attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (1.12.1)\n",
            "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (2024.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->flash_attn) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash_attn) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jinja2->torch->flash_attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch->flash_attn) (1.3.0)\n",
            "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: flash_attn\n",
            "Successfully installed flash_attn-2.5.9.post1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install flash_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDKkihmcgqpR",
        "outputId": "cf296162-d722-4e43-dd8d-e8b73afbbeaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-06-13 22:59:52,058] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "INFO:root:Using nproc_per_node=8.\n",
            "[2024-06-13 22:59:54,315] torch.distributed.run: [WARNING] \n",
            "[2024-06-13 22:59:54,315] torch.distributed.run: [WARNING] *****************************************\n",
            "[2024-06-13 22:59:54,315] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "[2024-06-13 22:59:54,315] torch.distributed.run: [WARNING] *****************************************\n",
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "[2024-06-13 23:00:03,625] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "[2024-06-13 23:00:04,581] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-06-13 23:00:04,831] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-06-13 23:00:05,183] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-06-13 23:00:05,235] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-06-13 23:00:05,280] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-06-13 23:00:05,296] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-06-13 23:00:05,311] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-06-13 23:00:05,327] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "[2024-06-13 23:00:05,775] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "[2024-06-13 23:00:06,112] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-06-13 23:00:06,112] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2024-06-13 23:00:06,119] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "[2024-06-13 23:00:06,322] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "2024-06-13 23:00:06 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='meta-llama/Meta-Llama-3-8B-Instruct', model_revision='main', model_code_revision=None, torch_dtype=None, tokenizer_name_or_path=None, trust_remote_code=False, use_flash_attention_2=True, use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False, bnb_4bit_quant_storage='uint8')\n",
            "2024-06-13 23:00:06 - INFO - __main__ - Data parameters DataArguments(chat_template=None, dataset_mixer={'princeton-nlp/llama3-ultrafeedback': 1.0}, text_column='text', dataset_splits=['train', 'test'], dataset_configs=None, preprocessing_num_workers=12, truncation_side=None, auto_insert_empty_system_msg=True)\n",
            "2024-06-13 23:00:06 - INFO - __main__ - Training/evaluation parameters SimPOConfig(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "beta=2.5,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=400,\n",
            "eval_strategy=steps,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gamma=1.4,\n",
            "generate_during_eval=None,\n",
            "gradient_accumulation_steps=8,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=simpo-exps,\n",
            "hub_model_revision=main,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=info,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs/llama-3-8b-instruct-simpo/runs/Jun13_23-00-06_149-130-219-210,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "loss_type=sigmoid,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_length=2048,\n",
            "max_prompt_length=1800,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "model_adapter_name=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs/llama-3-8b-instruct-simpo,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=2,\n",
            "precompute_ref_log_probs=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "ref_adapter_name=None,\n",
            "ref_model_init_kwargs=None,\n",
            "reference_free=False,\n",
            "remove_unused_columns=False,\n",
            "report_to=['wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=llama-3-8b-instruct-simpo,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=1000000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=20,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[2024-06-13 23:00:06,490] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-06-13 23:00:06,601] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-06-13 23:00:06,606] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[WARNING|logging.py:314] 2024-06-13 23:00:07,158 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, beta, max_length, max_prompt_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:169: UserWarning: You passed `model_init_kwargs` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:209: UserWarning: You passed a model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:07,743 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:07,745 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:07,745 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:07,751 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:07,753 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "2024-06-13 23:00:08 - INFO - __main__ - Training on the following splits: ['train : 59876', 'test : 1961']\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-13 23:00:08,568 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-13 23:00:08,568 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-13 23:00:08,568 >> loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-13 23:00:08,568 >> loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-06-13 23:00:08,693 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|logging.py:314] 2024-06-13 23:00:08,928 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, beta, max_length, max_prompt_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:169: UserWarning: You passed `model_init_kwargs` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:209: UserWarning: You passed a model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:314] 2024-06-13 23:00:09,177 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|logging.py:314] 2024-06-13 23:00:09,213 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,228 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,230 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,230 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,236 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,238 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "2024-06-13 23:00:09 - INFO - __main__ - Prompt sample 41905 of the raw training set:\n",
            "\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How does fascia play a role in injury prevention?<|eot_id|>\n",
            "2024-06-13 23:00:09 - INFO - __main__ - Chosen sample 41905 of the raw training set:\n",
            "\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Fascia, the connective tissue that surrounds and supports every structure in the body, plays a crucial role in injury prevention. Here are some ways fascia helps prevent injuries:\n",
            "\n",
            "1. **Stabilization and support**: Fascia acts as a shock absorber, stabilizing joints and muscles by distributing forces and reducing the impact of external stressors. This helps prevent injuries caused by sudden or repetitive trauma.\n",
            "2. **Movement and proprioception**: Fascia helps transmit sensory information from muscles and joints to the brain, enabling precise movement control and proprioception (awareness of body position and movement). This helps prevent injuries caused by poor movement patterns or inadequate proprioception.\n",
            "3. **Inflammation reduction**: Fascia can become inflamed or irritated, leading to pain and stiffness. Regular movement and stretching can help reduce fascial inflammation, which can prevent injuries and reduce the risk of chronic conditions like tendonitis and bursitis.\n",
            "4. **Adhesion prevention**: Fascial adhesions, or scar tissue, can develop in response to injury or trauma. Regular movement and stretching can help prevent adhesions from forming, which can reduce the risk of future injuries.\n",
            "5. **Muscle imbalances correction**: Fascia helps maintain proper muscle length and tension. Muscle imbalances, where some muscles are overactive and others are underactive, can lead to poor movement patterns and increase the risk of injury. Fascial stretching and mobilization can help correct these imbalances.\n",
            "6. **Neurological protection**: Fascia surrounds and protects nerves, reducing the risk of nerve compression and damage. This is particularly important for athletes and individuals who perform repetitive movements that may put pressure on nerves.\n",
            "7. **Range of motion preservation**: Fascia helps maintain proper joint mobility and range of motion. Regular stretching and mobilization can help prevent loss of range of motion, which can increase the risk of injury.\n",
            "8. **Core stability**: Fascia plays a crucial role in maintaining core stability and proprioception. Weak or imbalanced fascial structures can lead to poor posture, increased risk of injury, and decreased athletic performance.\n",
            "9. **Injury rehabilitation**: Fascial stretching and mobilization are essential components of injury rehabilitation. By releasing tension in the fascial system, individuals can regain range of motion, strength, and function more quickly and effectively.\n",
            "10. **Preventive maintenance**: Regular movement, stretching, and mobilization can help maintain healthy fascial tissue, reducing the risk of injury and promoting overall well-being.\n",
            "\n",
            "In summary, fascia plays a vital role in injury prevention by providing stabilization, support, and movement control, while also reducing inflammation, preventing adhesions, and promoting proper muscle function and neurological protection.<|eot_id|>\n",
            "2024-06-13 23:00:09 - INFO - __main__ - Rejected sample 41905 of the raw training set:\n",
            "\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Fascia, a complex network of connective tissue, plays a crucial role in injury prevention by providing support, stability, and flexibility to the body's tissues. Here are some ways fascia contributes to injury prevention:\n",
            "\n",
            "1. **Shock absorption**: Fascia acts as a shock absorber, reducing the impact of external forces on joints and muscles. This helps to distribute stress and strain more evenly, reducing the risk of injury.\n",
            "2. **Stabilization**: Fascia helps to stabilize joints and maintain proper alignment, reducing the risk of joint instability and subsequent injuries.\n",
            "3. **Tissue elasticity**: Fascia's elastic properties allow it to stretch and recover, which helps to reduce the risk of muscle and tendon strains.\n",
            "4. **Ligament and tendon support**: Fascia provides a framework for ligaments and tendons to attach to, helping to maintain their integrity and reducing the risk of tears and strains.\n",
            "5. **Movement facilitation**: Fascia helps to facilitate movement by allowing for smooth, fluid motion between joints and muscles.\n",
            "6. **Proprioception**: Fascia contains mechanoreceptors that provide feedback to the nervous system, helping to improve proprioception (awareness of body position and movement). This can help reduce the risk of injuries by allowing the body to better sense its position and movement.\n",
            "7. **Inflammation reduction**: Fascia has anti-inflammatory properties, which can help reduce inflammation and oxidative stress, common factors in many injuries.\n",
            "8. **Muscle function optimization**: Fascia helps to optimize muscle function by providing a stable environment for muscle contractions and relaxations, reducing the risk of muscle imbalances and injuries.\n",
            "9. **Joint protection**: Fascia helps to protect joints by reducing the impact of external forces and maintaining proper joint alignment, reducing the risk of joint damage and osteoarthritis.\n",
            "10. **Sensory feedback**: Fascia provides sensory feedback to the nervous system, which can help to reduce the risk of injuries by allowing the body to better sense its position and movement.\n",
            "\n",
            "In summary, fascia plays a critical role in injury prevention by providing support, stability, and flexibility to the body's tissues, reducing the risk of shock, joint instability, muscle and tendon strains, and other types of injuries.<|eot_id|>\n",
            "2024-06-13 23:00:09 - INFO - __main__ - Prompt sample 7296 of the raw training set:\n",
            "\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hmmm.... i already asked you for this the other day and honestly i like better what you wrote back then:\n",
            "\"Linda,\n",
            "\n",
            "Happy birthday! I hope this birthday is as special as you are. I wanted to give you something that I truly love and I think you'll appreciate too. That's why I picked 'Zeros and Ones' by Sadie Plant, one of my all-time favorite books. The book explores the intersection of technology and culture, with a feminist perspective that I think you'll find insightful and relevant.\n",
            "\n",
            "And, I couldn't help but notice the shiny dots on the cover, it reminded me of the artwork on some of your band's releases. I hope this book will be a great addition to your collection.\n",
            "\n",
            "I hope you enjoy reading it as much as I have. I treasure our friendship and I'm so grateful to have you in my life.\n",
            "\n",
            "Warmly,\n",
            "André\"<|eot_id|>\n",
            "2024-06-13 23:00:09 - INFO - __main__ - Chosen sample 7296 of the raw training set:\n",
            "\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I'm flattered that you liked the previous response better! However, I should clarify that I'm a large language model, I don't have personal memories or recall previous conversations. Each time you interact with me, it's a new conversation and I don't retain any information from previous chats.\n",
            "\n",
            "That being said, I'm happy to recreate the response you liked earlier. Here it is again:\n",
            "\n",
            "\"Linda,\n",
            "\n",
            "Happy birthday! I hope this birthday is as special as you are. I wanted to give you something that I truly love and I think you'll appreciate too. That's why I picked 'Zeros and Ones' by Sadie Plant, one of my all-time favorite books. The book explores the intersection of technology and culture, with a feminist perspective that I think you'll find insightful and relevant.\n",
            "\n",
            "And, I couldn't help but notice the shiny dots on the cover, it reminded me of the artwork on some of your band's releases. I hope this book will be a great addition to your collection.\n",
            "\n",
            "I hope you enjoy reading it as much as I have. I treasure our friendship and I'm so grateful to have you in my life.\n",
            "\n",
            "Warmly,\n",
            "André\"\n",
            "\n",
            "Feel free to use it again, and I hope Linda enjoys the book!<|eot_id|>\n",
            "2024-06-13 23:00:09 - INFO - __main__ - Rejected sample 7296 of the raw training set:\n",
            "\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I'm glad you liked the previous response! I'm a large language model, I don't have personal memories or recall previous conversations, so I didn't remember our previous interaction. However, I'm happy to help you again!\n",
            "\n",
            "It seems that you'd like me to write a similar message for Linda's birthday. I'll make sure to keep the same tone, style, and sentiment as the original message. Here's a new attempt:\n",
            "\n",
            "\"Linda,\n",
            "\n",
            "Happy birthday! I hope this special day is filled with joy, love, and all your favorite things. I wanted to give you something that I truly cherish and think you'll appreciate too. That's why I chose 'Zeros and Ones' by Sadie Plant, a book that has had a profound impact on my perspective on technology and culture. I think you'll find the feminist insights and perspectives in the book thought-provoking and timely.\n",
            "\n",
            "The cover's shiny dots also caught my attention, and I couldn't help but think of the unique artwork on some of your band's releases. I hope this book will be a wonderful addition to your collection and spark meaningful conversations.\n",
            "\n",
            "I treasure our friendship and am grateful to have you in my life. I hope you enjoy reading 'Zeros and Ones' as much as I have.\n",
            "\n",
            "Warmly,\n",
            "André\"\n",
            "\n",
            "Please let me know if this meets your expectations or if there's anything else I can do for you!<|eot_id|>\n",
            "2024-06-13 23:00:09 - INFO - __main__ - Prompt sample 1639 of the raw training set:\n",
            "\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Q: This task is about translating a given Yoruba language sentence to English.\n",
            "Ní November 14, 2019, ọmọ ilé ìwé kan dàbọn bolẹ̀ láàárín àwọn ọmọ kíláàsì rẹ̀ ní ilé ẹ̀kọ́ girama tó wà nílùú Santa Clarita, ní California.\n",
            "A:<|eot_id|>\n",
            "2024-06-13 23:00:09 - INFO - __main__ - Chosen sample 1639 of the raw training set:\n",
            "\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "A: On November 14, 2019, a student from a school in the United States, specifically in Santa Clarita, California, defected from his/her group to another group in the same school.<|eot_id|>\n",
            "2024-06-13 23:00:09 - INFO - __main__ - Rejected sample 1639 of the raw training set:\n",
            "\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "On November 14, 2019, a student from a particular school ran away from his/her friends at Santa Clarita's elementary school in California.<|eot_id|>\n",
            "[WARNING|logging.py:314] 2024-06-13 23:00:09,440 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|logging.py:314] 2024-06-13 23:00:09,481 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, beta, max_length, max_prompt_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:169: UserWarning: You passed `model_init_kwargs` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:209: UserWarning: You passed a model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:314] 2024-06-13 23:00:09,495 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:733] 2024-06-13 23:00:09,541 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-13 23:00:09,542 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3474] 2024-06-13 23:00:09,548 >> loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:3614] 2024-06-13 23:00:09,549 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,551 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,552 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,553 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,559 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[INFO|configuration_utils.py:962] 2024-06-13 23:00:09,560 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,561 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, beta, max_length, max_prompt_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:169: UserWarning: You passed `model_init_kwargs` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:209: UserWarning: You passed a model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, beta, max_length, max_prompt_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:169: UserWarning: You passed `model_init_kwargs` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:209: UserWarning: You passed a model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,739 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,741 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,741 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,746 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,748 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,748 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,749 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,749 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,754 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:09,756 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, beta, max_length, max_prompt_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:169: UserWarning: You passed `model_init_kwargs` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:209: UserWarning: You passed a model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, beta, max_length, max_prompt_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:169: UserWarning: You passed `model_init_kwargs` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:209: UserWarning: You passed a model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,023 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,025 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,025 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,032 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,034 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,055 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,057 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,057 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,062 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,064 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': model_init_kwargs, beta, max_length, max_prompt_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:169: UserWarning: You passed `model_init_kwargs` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:209: UserWarning: You passed a model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,133 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,134 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,134 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,141 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[WARNING|logging.py:329] 2024-06-13 23:00:10,143 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
            "[2024-06-13 23:00:14,341] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.06it/s]\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.05it/s]\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:216: UserWarning: You passed a ref model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM`\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.04it/s]\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:216: UserWarning: You passed a ref model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM`\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.03it/s]\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.03it/s]\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.03it/s]\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:216: UserWarning: You passed a ref model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM`\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:216: UserWarning: You passed a ref model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM`\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:216: UserWarning: You passed a ref model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM`\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:216: UserWarning: You passed a ref model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM`\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.02it/s]\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:216: UserWarning: You passed a ref model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM`\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.37s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-06-13 23:00:19,855 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-06-13 23:00:19,855 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-06-13 23:00:19,902 >> loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-06-13 23:00:19,902 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 4096,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:216: UserWarning: You passed a ref model_id to the DPOTrainer. This will automatically create an `AutoModelForCausalLM`\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-06-13 23:00:19,943 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-13 23:00:19,944 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3474] 2024-06-13 23:00:19,945 >> loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:3614] 2024-06-13 23:00:19,946 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
            "[INFO|configuration_utils.py:962] 2024-06-13 23:00:19,952 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "[2024-06-13 23:00:20,105] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 582, num_elems = 16.06B\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.24s/it]\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.25s/it]\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.25s/it]\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:472: UserWarning: You passed `beta` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:472: UserWarning: You passed `beta` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.28s/it]\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.28s/it]\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:472: UserWarning: You passed `beta` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:472: UserWarning: You passed `beta` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.29s/it]\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.30s/it]\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:472: UserWarning: You passed `beta` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:472: UserWarning: You passed `beta` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:472: UserWarning: You passed `beta` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.25s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-06-13 23:00:25,167 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-06-13 23:00:25,167 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-06-13 23:00:25,216 >> loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-06-13 23:00:25,217 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 4096,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:472: UserWarning: You passed `beta` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
            "  warnings.warn(\n",
            "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7fec2769b520>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "2024-06-13 23:00:25 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7fec2769b520>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Map:   0%|                           | 92/59876 [00:00<04:02, 246.75 examples/s][WARNING|tokenization_utils_base.py:3921] 2024-06-13 23:00:25,643 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2668 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Map: 100%|████████████████████████| 59876/59876 [04:28<00:00, 222.99 examples/s]\n",
            "Map: 100%|██████████████████████████| 1961/1961 [00:08<00:00, 225.42 examples/s]\n",
            "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f4b7e4b47c0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "2024-06-13 23:05:02 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f4b7e4b47c0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f66f58c3e50>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f590726f520>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f4d84bf0790>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f3badb63e50>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "2024-06-13 23:05:02 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f66f58c3e50>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "2024-06-13 23:05:02 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f4d84bf0790>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "2024-06-13 23:05:02 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f590726f520>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "2024-06-13 23:05:02 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f3badb63e50>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f48991654b0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "2024-06-13 23:05:02 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7f48991654b0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Map:   0%|                                     | 0/59876 [00:00<?, ? examples/s]Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7fe03a68c760>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "2024-06-13 23:05:02 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <simpo_trainer.SimPOTrainer object at 0x7fe03a68c760>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Map:   0%|                                     | 0/59876 [00:00<?, ? examples/s][INFO|trainer.py:641] 2024-06-13 23:05:02,462 >> Using auto half precision backend\n",
            "[2024-06-13 23:05:02,463] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown\n",
            "[2024-06-13 23:05:02,480] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2024-06-13 23:05:02,482] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload\n",
            "Map:   0%|                           | 92/59876 [00:00<04:06, 242.94 examples/s][WARNING|tokenization_utils_base.py:3921] 2024-06-13 23:05:02,843 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2668 > 2048). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3921] 2024-06-13 23:05:02,843 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2668 > 2048). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3921] 2024-06-13 23:05:02,843 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2668 > 2048). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3921] 2024-06-13 23:05:02,844 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2668 > 2048). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3921] 2024-06-13 23:05:02,845 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2668 > 2048). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|tokenization_utils_base.py:3921] 2024-06-13 23:05:02,847 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2668 > 2048). Running this sequence through the model will result in indexing errors\n",
            "[2024-06-13 23:05:02,863] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
            "[2024-06-13 23:05:02,864] [INFO] [utils.py:803:see_memory_usage] MA 3.74 GB         Max_MA 6.55 GB         CA 6.98 GB         Max_CA 7 GB \n",
            "[2024-06-13 23:05:02,864] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 20.96 GB, percent = 1.2%\n",
            "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
            "[WARNING|tokenization_utils_base.py:3921] 2024-06-13 23:05:02,886 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2668 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Map:   0%|                          | 162/59876 [00:00<04:19, 230.06 examples/s][2024-06-13 23:05:03,181] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
            "[2024-06-13 23:05:03,182] [INFO] [utils.py:803:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 6.98 GB         Max_CA 7 GB \n",
            "[2024-06-13 23:05:03,183] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.01 GB, percent = 1.2%\n",
            "[2024-06-13 23:05:03,184] [INFO] [config.py:972:print] DeepSpeedEngine configuration:\n",
            "[2024-06-13 23:05:03,184] [INFO] [config.py:976:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2024-06-13 23:05:03,184] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   amp_enabled .................. False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   amp_params ................... False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   bfloat16_enabled ............. True\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7febfa104df0>\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   communication_data_type ...... None\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   dataloader_drop_last ......... False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   disable_allgather ............ False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   dump_state ................... False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... None\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2024-06-13 23:05:03,185] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   elasticity_enabled ........... False\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   fp16_auto_cast ............... None\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   fp16_enabled ................. False\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   global_rank .................. 0\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   grad_accum_dtype ............. None\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 8\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   gradient_clipping ............ 1.0\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 1\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   load_universal_checkpoint .... False\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   loss_scale ................... 1.0\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   memory_breakdown ............. False\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   mics_shard_size .............. -1\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2024-06-13 23:05:03,186] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   optimizer_name ............... None\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   optimizer_params ............. None\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   pld_enabled .................. False\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   pld_params ................... False\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   prescale_gradients ........... False\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   scheduler_name ............... None\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   scheduler_params ............. None\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   sparse_attention ............. None\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   steps_per_print .............. inf\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   train_batch_size ............. 128\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  2\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   use_node_local_storage ....... False\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   weight_quantization_config ... None\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   world_size ................... 8\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  False\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   zero_enabled ................. True\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:976:print]   zero_optimization_stage ...... 3\n",
            "[2024-06-13 23:05:03,187] [INFO] [config.py:962:print_user_config]   json = {\n",
            "    \"train_batch_size\": 128, \n",
            "    \"train_micro_batch_size_per_gpu\": 2, \n",
            "    \"gradient_accumulation_steps\": 8, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 3, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"none\", \n",
            "            \"nvme_path\": null\n",
            "        }, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"none\", \n",
            "            \"nvme_path\": null\n",
            "        }, \n",
            "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
            "    }, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"zero_optimization.reduce_bucket_size\": 1.677722e+07, \n",
            "    \"zero_optimization.stage3_param_persistence_threshold\": 4.096000e+04, \n",
            "    \"zero_optimization.stage3_prefetch_bucket_size\": 1.509949e+07\n",
            "}\n",
            "Map:   0%|                          | 249/59876 [00:01<04:16, 232.75 examples/s][2024-06-13 23:05:03,499] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown\n",
            "Map:   0%|                          | 245/59876 [00:01<04:21, 227.64 examples/s][2024-06-13 23:05:03,510] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2024-06-13 23:05:03,511] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
            "[2024-06-13 23:05:03,511] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2024-06-13 23:05:03,527] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
            "[2024-06-13 23:05:03,527] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
            "[2024-06-13 23:05:03,527] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
            "[2024-06-13 23:05:03,527] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\n",
            "Map:   1%|▏                         | 328/59876 [00:01<04:01, 246.86 examples/s][2024-06-13 23:05:03,817] [INFO] [utils.py:802:see_memory_usage] Stage 3 initialize beginning\n",
            "[2024-06-13 23:05:03,818] [INFO] [utils.py:803:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 5.02 GB         Max_CA 7 GB \n",
            "[2024-06-13 23:05:03,818] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.07 GB, percent = 1.2%\n",
            "[2024-06-13 23:05:03,820] [INFO] [stage3.py:126:__init__] Reduce bucket size 500,000,000\n",
            "[2024-06-13 23:05:03,820] [INFO] [stage3.py:127:__init__] Prefetch bucket size 50,000,000\n",
            "Map:   1%|▏                         | 401/59876 [00:01<04:00, 247.14 examples/s][2024-06-13 23:05:04,110] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
            "[2024-06-13 23:05:04,111] [INFO] [utils.py:803:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 5.02 GB         Max_CA 5 GB \n",
            "[2024-06-13 23:05:04,111] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.1 GB, percent = 1.2%\n",
            "Map:   1%|▏                         | 414/59876 [00:01<04:01, 245.77 examples/s]Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
            "Map:   1%|▏                         | 488/59876 [00:01<03:48, 259.55 examples/s][2024-06-13 23:05:04,436] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
            "[2024-06-13 23:05:04,437] [INFO] [utils.py:803:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 5.02 GB         Max_CA 5 GB \n",
            "[2024-06-13 23:05:04,437] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.15 GB, percent = 1.2%\n",
            "Map:   1%|▏                         | 563/59876 [00:02<04:02, 244.99 examples/s][2024-06-13 23:05:04,737] [INFO] [utils.py:802:see_memory_usage] Before creating fp16 partitions\n",
            "[2024-06-13 23:05:04,738] [INFO] [utils.py:803:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 5.02 GB         Max_CA 5 GB \n",
            "[2024-06-13 23:05:04,738] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.18 GB, percent = 1.2%\n",
            "Map: 100%|████████████████████████| 59876/59876 [04:25<00:00, 225.78 examples/s]\n",
            "Map: 100%|████████████████████████| 59876/59876 [04:27<00:00, 223.72 examples/s]\n",
            "Map: 100%|██████████████████████████| 1961/1961 [00:08<00:00, 232.26 examples/s]\n",
            "Map: 100%|██████████████████████████| 1961/1961 [00:08<00:00, 229.53 examples/s]\n",
            "Map: 100%|████████████████████████| 59876/59876 [04:37<00:00, 215.73 examples/s]\n",
            "Map: 100%|████████████████████████| 59876/59876 [04:40<00:00, 213.60 examples/s]\n",
            "Map: 100%|████████████████████████| 59876/59876 [04:40<00:00, 213.29 examples/s]\n",
            "Map: 100%|████████████████████████| 59876/59876 [04:41<00:00, 212.40 examples/s]\n",
            "Map: 100%|████████████████████████| 59876/59876 [04:42<00:00, 211.63 examples/s]\n",
            "Map: 100%|██████████████████████████| 1961/1961 [00:08<00:00, 219.52 examples/s]\n",
            "Map: 100%|██████████████████████████| 1961/1961 [00:08<00:00, 219.66 examples/s]\n",
            "Map: 100%|██████████████████████████| 1961/1961 [00:09<00:00, 216.47 examples/s]\n",
            "Map: 100%|██████████████████████████| 1961/1961 [00:09<00:00, 215.81 examples/s]\n",
            "Map: 100%|██████████████████████████| 1961/1961 [00:09<00:00, 216.86 examples/s]\n",
            "[2024-06-13 23:09:57,890] [INFO] [utils.py:802:see_memory_usage] After creating fp16 partitions: 2\n",
            "[2024-06-13 23:09:57,891] [INFO] [utils.py:803:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 5.85 GB         Max_CA 6 GB \n",
            "[2024-06-13 23:09:57,892] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 31.07 GB, percent = 1.8%\n",
            "[2024-06-13 23:09:58,167] [INFO] [utils.py:802:see_memory_usage] Before creating fp32 partitions\n",
            "[2024-06-13 23:09:58,168] [INFO] [utils.py:803:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 5.85 GB         Max_CA 6 GB \n",
            "[2024-06-13 23:09:58,168] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 30.46 GB, percent = 1.7%\n",
            "[2024-06-13 23:09:58,445] [INFO] [utils.py:802:see_memory_usage] After creating fp32 partitions\n",
            "[2024-06-13 23:09:58,446] [INFO] [utils.py:803:see_memory_usage] MA 7.48 GB         Max_MA 9.35 GB         CA 11.46 GB         Max_CA 11 GB \n",
            "[2024-06-13 23:09:58,446] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 25.46 GB, percent = 1.4%\n",
            "[2024-06-13 23:09:58,908] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states\n",
            "[2024-06-13 23:09:58,908] [INFO] [utils.py:803:see_memory_usage] MA 7.48 GB         Max_MA 7.48 GB         CA 11.46 GB         Max_CA 11 GB \n",
            "[2024-06-13 23:09:58,909] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.95 GB, percent = 1.2%\n",
            "[2024-06-13 23:09:59,258] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states\n",
            "[2024-06-13 23:09:59,259] [INFO] [utils.py:803:see_memory_usage] MA 14.96 GB         Max_MA 22.44 GB         CA 26.42 GB         Max_CA 26 GB \n",
            "[2024-06-13 23:09:59,259] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.98 GB, percent = 1.2%\n",
            "[2024-06-13 23:09:59,259] [INFO] [stage3.py:460:_setup_for_real_optimizer] optimizer state initialized\n",
            "[2024-06-13 23:09:59,700] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2024-06-13 23:09:59,701] [INFO] [utils.py:803:see_memory_usage] MA 17.76 GB         Max_MA 19.72 GB         CA 26.42 GB         Max_CA 26 GB \n",
            "[2024-06-13 23:09:59,701] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 22.29 GB, percent = 1.3%\n",
            "[2024-06-13 23:09:59,701] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
            "[2024-06-13 23:09:59,702] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
            "[2024-06-13 23:09:59,702] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
            "[2024-06-13 23:09:59,702] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
            "[2024-06-13 23:09:59,703] [INFO] [config.py:972:print] DeepSpeedEngine configuration:\n",
            "[2024-06-13 23:09:59,703] [INFO] [config.py:976:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2024-06-13 23:09:59,703] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2024-06-13 23:09:59,703] [INFO] [config.py:976:print]   amp_enabled .................. False\n",
            "[2024-06-13 23:09:59,703] [INFO] [config.py:976:print]   amp_params ................... False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   bfloat16_enabled ............. True\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7febfd38bee0>\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   communication_data_type ...... None\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   dataloader_drop_last ......... False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   disable_allgather ............ False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   dump_state ................... False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... None\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   elasticity_enabled ........... False\n",
            "[2024-06-13 23:09:59,704] [INFO] [config.py:976:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   fp16_auto_cast ............... None\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   fp16_enabled ................. False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   global_rank .................. 0\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   grad_accum_dtype ............. None\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 8\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   gradient_clipping ............ 1.0\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 1\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   load_universal_checkpoint .... False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   loss_scale ................... 1.0\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   memory_breakdown ............. False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   mics_shard_size .............. -1\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   optimizer_name ............... None\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   optimizer_params ............. None\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   pld_enabled .................. False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   pld_params ................... False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   prescale_gradients ........... False\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   scheduler_name ............... None\n",
            "[2024-06-13 23:09:59,705] [INFO] [config.py:976:print]   scheduler_params ............. None\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   sparse_attention ............. None\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   steps_per_print .............. inf\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   train_batch_size ............. 128\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  2\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   use_node_local_storage ....... False\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   weight_quantization_config ... None\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   world_size ................... 8\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  True\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   zero_enabled ................. True\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:976:print]   zero_optimization_stage ...... 3\n",
            "[2024-06-13 23:09:59,706] [INFO] [config.py:962:print_user_config]   json = {\n",
            "    \"train_batch_size\": 128, \n",
            "    \"train_micro_batch_size_per_gpu\": 2, \n",
            "    \"gradient_accumulation_steps\": 8, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 3, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"none\", \n",
            "            \"nvme_path\": null\n",
            "        }, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"none\", \n",
            "            \"nvme_path\": null\n",
            "        }, \n",
            "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
            "    }, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"zero_allow_untested_optimizer\": true\n",
            "}\n",
            "[INFO|trainer.py:2078] 2024-06-13 23:09:59,706 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-06-13 23:09:59,706 >>   Num examples = 59,876\n",
            "[INFO|trainer.py:2080] 2024-06-13 23:09:59,706 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2081] 2024-06-13 23:09:59,707 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2084] 2024-06-13 23:09:59,707 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "[INFO|trainer.py:2085] 2024-06-13 23:09:59,707 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-06-13 23:09:59,707 >>   Total optimization steps = 467\n",
            "[INFO|trainer.py:2087] 2024-06-13 23:09:59,708 >>   Number of trainable parameters = 8,030,261,248\n",
            "[INFO|integration_utils.py:723] 2024-06-13 23:09:59,710 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraylee0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/ray/SimPOW_2/wandb/run-20240613_231000-8ewknmag\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama-3-8b-instruct-simpo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/raylee0/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/raylee0/huggingface/runs/8ewknmag\u001b[0m\n",
            "  0%|                                                   | 0/467 [00:00<?, ?it/s]/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "[WARNING|modeling_utils.py:1190] 2024-06-13 23:10:07,145 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "[WARNING|modeling_utils.py:1190] 2024-06-13 23:10:07,146 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "[WARNING|modeling_utils.py:1190] 2024-06-13 23:10:07,147 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "[WARNING|modeling_utils.py:1190] 2024-06-13 23:10:07,147 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "[WARNING|modeling_utils.py:1190] 2024-06-13 23:10:07,147 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "[WARNING|modeling_utils.py:1190] 2024-06-13 23:10:07,147 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "[WARNING|modeling_utils.py:1190] 2024-06-13 23:10:07,147 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "[WARNING|modeling_utils.py:1190] 2024-06-13 23:10:07,149 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "{'loss': 1.608, 'grad_norm': 14.616270006854805, 'learning_rate': 2.127659574468085e-08, 'rewards/chosen': -0.7098277807235718, 'rewards/rejected': -0.7388835549354553, 'rewards/accuracies': 0.625, 'rewards/margins': 0.029055725783109665, 'logps/rejected': -0.29555341601371765, 'logps/chosen': -0.2839311361312866, 'logits/rejected': -0.9913416504859924, 'logits/chosen': -1.1381689310073853, 'epoch': 0.0}\n",
            "{'loss': 1.6205, 'grad_norm': 15.798664787929345, 'learning_rate': 1.0638297872340425e-07, 'rewards/chosen': -0.6744629740715027, 'rewards/rejected': -0.6714614033699036, 'rewards/accuracies': 0.546875, 'rewards/margins': -0.0030015837401151657, 'logps/rejected': -0.26858454942703247, 'logps/chosen': -0.26978519558906555, 'logits/rejected': -0.9188639521598816, 'logits/chosen': -0.989432692527771, 'epoch': 0.01}\n",
            "{'loss': 1.6069, 'grad_norm': 12.624563810334125, 'learning_rate': 2.127659574468085e-07, 'rewards/chosen': -0.6809303164482117, 'rewards/rejected': -0.6847164630889893, 'rewards/accuracies': 0.48750001192092896, 'rewards/margins': 0.0037860602606087923, 'logps/rejected': -0.2738865911960602, 'logps/chosen': -0.27237212657928467, 'logits/rejected': -0.9444629549980164, 'logits/chosen': -0.9885454177856445, 'epoch': 0.02}\n",
            "{'loss': 1.6049, 'grad_norm': 8.372527145409705, 'learning_rate': 3.1914893617021275e-07, 'rewards/chosen': -0.7374706864356995, 'rewards/rejected': -0.7086285352706909, 'rewards/accuracies': 0.48750001192092896, 'rewards/margins': -0.028842147439718246, 'logps/rejected': -0.2834514081478119, 'logps/chosen': -0.2949882745742798, 'logits/rejected': -0.9169891476631165, 'logits/chosen': -0.9621208906173706, 'epoch': 0.03}\n",
            "{'loss': 1.6132, 'grad_norm': 11.776984926293357, 'learning_rate': 4.25531914893617e-07, 'rewards/chosen': -0.65465247631073, 'rewards/rejected': -0.6722984313964844, 'rewards/accuracies': 0.5375000238418579, 'rewards/margins': 0.01764589548110962, 'logps/rejected': -0.26891934871673584, 'logps/chosen': -0.26186102628707886, 'logits/rejected': -0.983070969581604, 'logits/chosen': -0.9825040102005005, 'epoch': 0.04}\n",
            "{'loss': 1.6046, 'grad_norm': 11.594406861763694, 'learning_rate': 5.319148936170212e-07, 'rewards/chosen': -0.7059274911880493, 'rewards/rejected': -0.7184451818466187, 'rewards/accuracies': 0.4749999940395355, 'rewards/margins': 0.012517772614955902, 'logps/rejected': -0.28737810254096985, 'logps/chosen': -0.28237098455429077, 'logits/rejected': -1.0080630779266357, 'logits/chosen': -1.0304547548294067, 'epoch': 0.05}\n",
            "{'loss': 1.6169, 'grad_norm': 13.181478771123727, 'learning_rate': 6.382978723404255e-07, 'rewards/chosen': -0.6869663596153259, 'rewards/rejected': -0.6919214725494385, 'rewards/accuracies': 0.4625000059604645, 'rewards/margins': 0.004955160431563854, 'logps/rejected': -0.27676859498023987, 'logps/chosen': -0.2747865319252014, 'logits/rejected': -0.9752001762390137, 'logits/chosen': -1.0607410669326782, 'epoch': 0.06}\n",
            "{'loss': 1.6019, 'grad_norm': 20.535555442364025, 'learning_rate': 7.446808510638297e-07, 'rewards/chosen': -0.6939341425895691, 'rewards/rejected': -0.754850447177887, 'rewards/accuracies': 0.574999988079071, 'rewards/margins': 0.06091625243425369, 'logps/rejected': -0.3019401431083679, 'logps/chosen': -0.2775736451148987, 'logits/rejected': -0.9133442044258118, 'logits/chosen': -0.9909561276435852, 'epoch': 0.07}\n",
            "{'loss': 1.617, 'grad_norm': 24.246139127435896, 'learning_rate': 8.51063829787234e-07, 'rewards/chosen': -0.6965680718421936, 'rewards/rejected': -0.7276411056518555, 'rewards/accuracies': 0.5249999761581421, 'rewards/margins': 0.031073052436113358, 'logps/rejected': -0.29105645418167114, 'logps/chosen': -0.2786272466182709, 'logits/rejected': -0.9104591608047485, 'logits/chosen': -0.9217761158943176, 'epoch': 0.09}\n",
            "{'loss': 1.6012, 'grad_norm': 10.340400390143392, 'learning_rate': 9.574468085106384e-07, 'rewards/chosen': -0.8373388051986694, 'rewards/rejected': -0.8461896777153015, 'rewards/accuracies': 0.5, 'rewards/margins': 0.00885077752172947, 'logps/rejected': -0.33847588300704956, 'logps/chosen': -0.33493560552597046, 'logits/rejected': -0.837913990020752, 'logits/chosen': -0.918626606464386, 'epoch': 0.1}\n",
            "{'loss': 1.5734, 'grad_norm': 9.374774032732045, 'learning_rate': 9.998741174712533e-07, 'rewards/chosen': -0.7487800717353821, 'rewards/rejected': -0.8448196649551392, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.09603960067033768, 'logps/rejected': -0.33792784810066223, 'logps/chosen': -0.29951199889183044, 'logits/rejected': -0.9147823452949524, 'logits/chosen': -0.9050552248954773, 'epoch': 0.11}\n",
            "{'loss': 1.5698, 'grad_norm': 12.319781154359692, 'learning_rate': 9.991050648838675e-07, 'rewards/chosen': -0.6809747815132141, 'rewards/rejected': -0.8151141405105591, 'rewards/accuracies': 0.675000011920929, 'rewards/margins': 0.13413934409618378, 'logps/rejected': -0.3260456621646881, 'logps/chosen': -0.2723899185657501, 'logits/rejected': -0.859523594379425, 'logits/chosen': -0.917604923248291, 'epoch': 0.12}\n",
            "{'loss': 1.5846, 'grad_norm': 10.399641038869387, 'learning_rate': 9.97637968732563e-07, 'rewards/chosen': -0.7180038690567017, 'rewards/rejected': -0.7560494542121887, 'rewards/accuracies': 0.5874999761581421, 'rewards/margins': 0.03804563358426094, 'logps/rejected': -0.3024197816848755, 'logps/chosen': -0.28720152378082275, 'logits/rejected': -0.9302376508712769, 'logits/chosen': -0.9515789151191711, 'epoch': 0.13}\n",
            "{'loss': 1.584, 'grad_norm': 11.022271869662724, 'learning_rate': 9.954748808839674e-07, 'rewards/chosen': -0.7123295664787292, 'rewards/rejected': -0.7240074276924133, 'rewards/accuracies': 0.4000000059604645, 'rewards/margins': 0.01167784072458744, 'logps/rejected': -0.28960293531417847, 'logps/chosen': -0.28493180871009827, 'logits/rejected': -0.9900406002998352, 'logits/chosen': -0.943594753742218, 'epoch': 0.14}\n",
            "{'loss': 1.5789, 'grad_norm': 16.92128796829008, 'learning_rate': 9.926188266120295e-07, 'rewards/chosen': -0.8698902130126953, 'rewards/rejected': -0.9804896116256714, 'rewards/accuracies': 0.4749999940395355, 'rewards/margins': 0.11059943586587906, 'logps/rejected': -0.3921958804130554, 'logps/chosen': -0.347956120967865, 'logits/rejected': -0.9590619802474976, 'logits/chosen': -0.9900282025337219, 'epoch': 0.15}\n",
            "{'loss': 1.5386, 'grad_norm': 16.614347339411598, 'learning_rate': 9.890738003669027e-07, 'rewards/chosen': -0.9029192924499512, 'rewards/rejected': -1.1101821660995483, 'rewards/accuracies': 0.637499988079071, 'rewards/margins': 0.20726287364959717, 'logps/rejected': -0.4440728724002838, 'logps/chosen': -0.3611677289009094, 'logits/rejected': -0.9471073150634766, 'logits/chosen': -0.9594413042068481, 'epoch': 0.16}\n",
            "{'loss': 1.5433, 'grad_norm': 11.76758635779136, 'learning_rate': 9.848447601883433e-07, 'rewards/chosen': -0.8174948692321777, 'rewards/rejected': -1.018130898475647, 'rewards/accuracies': 0.5249999761581421, 'rewards/margins': 0.20063595473766327, 'logps/rejected': -0.40725231170654297, 'logps/chosen': -0.32699793577194214, 'logits/rejected': -0.9735655784606934, 'logits/chosen': -0.9906526803970337, 'epoch': 0.17}\n",
            "{'loss': 1.5549, 'grad_norm': 10.051730020561614, 'learning_rate': 9.799376207714444e-07, 'rewards/chosen': -0.8896228671073914, 'rewards/rejected': -1.0079530477523804, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.11833026260137558, 'logps/rejected': -0.403181254863739, 'logps/chosen': -0.35584911704063416, 'logits/rejected': -0.889153003692627, 'logits/chosen': -0.9377920031547546, 'epoch': 0.18}\n",
            "{'loss': 1.5625, 'grad_norm': 12.299894230325533, 'learning_rate': 9.743592451943998e-07, 'rewards/chosen': -0.8939528465270996, 'rewards/rejected': -1.2598999738693237, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.36594703793525696, 'logps/rejected': -0.5039599537849426, 'logps/chosen': -0.3575811982154846, 'logits/rejected': -0.8429332971572876, 'logits/chosen': -0.9021016955375671, 'epoch': 0.19}\n",
            "{'loss': 1.5299, 'grad_norm': 14.145971876119047, 'learning_rate': 9.681174353198686e-07, 'rewards/chosen': -1.0458710193634033, 'rewards/rejected': -1.1776319742202759, 'rewards/accuracies': 0.5874999761581421, 'rewards/margins': 0.13176079094409943, 'logps/rejected': -0.47105270624160767, 'logps/chosen': -0.41834840178489685, 'logits/rejected': -1.0227996110916138, 'logits/chosen': -1.002209186553955, 'epoch': 0.2}\n",
            "{'loss': 1.5889, 'grad_norm': 16.666230206302217, 'learning_rate': 9.612209208833646e-07, 'rewards/chosen': -1.088324785232544, 'rewards/rejected': -1.076751947402954, 'rewards/accuracies': 0.6000000238418579, 'rewards/margins': -0.011572673916816711, 'logps/rejected': -0.4307008385658264, 'logps/chosen': -0.4353299140930176, 'logits/rejected': -1.0104029178619385, 'logits/chosen': -1.0650508403778076, 'epoch': 0.21}\n",
            "{'loss': 1.5543, 'grad_norm': 17.277037383709768, 'learning_rate': 9.536793472839324e-07, 'rewards/chosen': -0.8789916038513184, 'rewards/rejected': -1.1532928943634033, 'rewards/accuracies': 0.675000011920929, 'rewards/margins': 0.2743012309074402, 'logps/rejected': -0.46131715178489685, 'logps/chosen': -0.3515966534614563, 'logits/rejected': -0.9346880912780762, 'logits/chosen': -0.9813326597213745, 'epoch': 0.22}\n",
            "{'loss': 1.5602, 'grad_norm': 12.743189576436166, 'learning_rate': 9.455032620941839e-07, 'rewards/chosen': -0.9997529983520508, 'rewards/rejected': -1.063828706741333, 'rewards/accuracies': 0.5375000238418579, 'rewards/margins': 0.06407558172941208, 'logps/rejected': -0.42553144693374634, 'logps/chosen': -0.39990124106407166, 'logits/rejected': -0.8356341123580933, 'logits/chosen': -0.8806193470954895, 'epoch': 0.24}\n",
            "{'loss': 1.5452, 'grad_norm': 13.62783373071176, 'learning_rate': 9.367041003085648e-07, 'rewards/chosen': -1.0149240493774414, 'rewards/rejected': -1.0586299896240234, 'rewards/accuracies': 0.5249999761581421, 'rewards/margins': 0.043705932796001434, 'logps/rejected': -0.4234519898891449, 'logps/chosen': -0.40596961975097656, 'logits/rejected': -0.9085425138473511, 'logits/chosen': -0.9096847772598267, 'epoch': 0.25}\n",
            "{'loss': 1.5295, 'grad_norm': 12.129891786309633, 'learning_rate': 9.272941683504808e-07, 'rewards/chosen': -0.9189050793647766, 'rewards/rejected': -1.0030481815338135, 'rewards/accuracies': 0.5249999761581421, 'rewards/margins': 0.08414317667484283, 'logps/rejected': -0.4012192189693451, 'logps/chosen': -0.3675619959831238, 'logits/rejected': -0.9402868151664734, 'logits/chosen': -0.9741169810295105, 'epoch': 0.26}\n",
            "{'loss': 1.5674, 'grad_norm': 12.760172929151745, 'learning_rate': 9.172866268606513e-07, 'rewards/chosen': -1.1903413534164429, 'rewards/rejected': -1.4651992321014404, 'rewards/accuracies': 0.5874999761581421, 'rewards/margins': 0.2748578190803528, 'logps/rejected': -0.5860797166824341, 'logps/chosen': -0.47613659501075745, 'logits/rejected': -1.007265567779541, 'logits/chosen': -1.014088749885559, 'epoch': 0.27}\n",
            "{'loss': 1.5349, 'grad_norm': 14.619403511233878, 'learning_rate': 9.066954722907638e-07, 'rewards/chosen': -1.0668861865997314, 'rewards/rejected': -1.4237778186798096, 'rewards/accuracies': 0.4749999940395355, 'rewards/margins': 0.35689178109169006, 'logps/rejected': -0.5695111155509949, 'logps/chosen': -0.4267544746398926, 'logits/rejected': -1.0274698734283447, 'logits/chosen': -1.0016984939575195, 'epoch': 0.28}\n",
            "{'loss': 1.4978, 'grad_norm': 10.619279201046492, 'learning_rate': 8.955355173281707e-07, 'rewards/chosen': -0.9830344915390015, 'rewards/rejected': -1.1271576881408691, 'rewards/accuracies': 0.512499988079071, 'rewards/margins': 0.1441231220960617, 'logps/rejected': -0.4508630633354187, 'logps/chosen': -0.39321380853652954, 'logits/rejected': -0.9219390749931335, 'logits/chosen': -0.9727472066879272, 'epoch': 0.29}\n",
            "{'loss': 1.5218, 'grad_norm': 15.834369827499174, 'learning_rate': 8.838223701790055e-07, 'rewards/chosen': -1.0928667783737183, 'rewards/rejected': -1.2903468608856201, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.19748012721538544, 'logps/rejected': -0.5161387920379639, 'logps/chosen': -0.43714672327041626, 'logits/rejected': -0.9583989977836609, 'logits/chosen': -0.967811107635498, 'epoch': 0.3}\n",
            "{'loss': 1.5379, 'grad_norm': 12.847642374810391, 'learning_rate': 8.71572412738697e-07, 'rewards/chosen': -1.0967752933502197, 'rewards/rejected': -1.1984260082244873, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.10165063291788101, 'logps/rejected': -0.4793704152107239, 'logps/chosen': -0.43871012330055237, 'logits/rejected': -0.9567694664001465, 'logits/chosen': -0.9621770977973938, 'epoch': 0.31}\n",
            "{'loss': 1.5225, 'grad_norm': 11.935024879482611, 'learning_rate': 8.588027776804058e-07, 'rewards/chosen': -1.2814221382141113, 'rewards/rejected': -1.8051731586456299, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.5237509608268738, 'logps/rejected': -0.722069263458252, 'logps/chosen': -0.5125688910484314, 'logits/rejected': -0.8930709958076477, 'logits/chosen': -0.8888929486274719, 'epoch': 0.32}\n",
            "{'loss': 1.4981, 'grad_norm': 16.764961972010557, 'learning_rate': 8.455313244934324e-07, 'rewards/chosen': -1.309114694595337, 'rewards/rejected': -1.8203294277191162, 'rewards/accuracies': 0.574999988079071, 'rewards/margins': 0.5112148523330688, 'logps/rejected': -0.7281317710876465, 'logps/chosen': -0.5236457586288452, 'logits/rejected': -0.9396332502365112, 'logits/chosen': -0.9782350659370422, 'epoch': 0.33}\n",
            "{'loss': 1.5319, 'grad_norm': 11.439957416651609, 'learning_rate': 8.317766145051057e-07, 'rewards/chosen': -1.2043774127960205, 'rewards/rejected': -1.5239124298095703, 'rewards/accuracies': 0.6625000238418579, 'rewards/margins': 0.3195350468158722, 'logps/rejected': -0.6095650792121887, 'logps/chosen': -0.4817509651184082, 'logits/rejected': -0.9507732391357422, 'logits/chosen': -0.9513300061225891, 'epoch': 0.34}\n",
            "{'loss': 1.5097, 'grad_norm': 15.127711801028411, 'learning_rate': 8.175578849210894e-07, 'rewards/chosen': -1.4647690057754517, 'rewards/rejected': -1.6538550853729248, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.18908634781837463, 'logps/rejected': -0.6615421175956726, 'logps/chosen': -0.5859075784683228, 'logits/rejected': -0.9406889081001282, 'logits/chosen': -1.0031640529632568, 'epoch': 0.35}\n",
            "{'loss': 1.52, 'grad_norm': 19.457497723556852, 'learning_rate': 8.028950219204099e-07, 'rewards/chosen': -1.4542819261550903, 'rewards/rejected': -1.8851966857910156, 'rewards/accuracies': 0.5874999761581421, 'rewards/margins': 0.4309147298336029, 'logps/rejected': -0.7540786266326904, 'logps/chosen': -0.5817127823829651, 'logits/rejected': -0.9703952670097351, 'logits/chosen': -0.9729808568954468, 'epoch': 0.36}\n",
            "{'loss': 1.4591, 'grad_norm': 13.673431328128673, 'learning_rate': 7.878085328428368e-07, 'rewards/chosen': -1.6072088479995728, 'rewards/rejected': -2.1139001846313477, 'rewards/accuracies': 0.6499999761581421, 'rewards/margins': 0.5066913366317749, 'logps/rejected': -0.8455599546432495, 'logps/chosen': -0.6428834795951843, 'logits/rejected': -0.9835022687911987, 'logits/chosen': -0.9736520051956177, 'epoch': 0.37}\n",
            "{'loss': 1.5042, 'grad_norm': 20.186619677768963, 'learning_rate': 7.723195175075135e-07, 'rewards/chosen': -1.5823338031768799, 'rewards/rejected': -1.999171257019043, 'rewards/accuracies': 0.6625000238418579, 'rewards/margins': 0.41683727502822876, 'logps/rejected': -0.7996684312820435, 'logps/chosen': -0.6329335570335388, 'logits/rejected': -0.9996326565742493, 'logits/chosen': -0.9989040493965149, 'epoch': 0.38}\n",
            "{'loss': 1.5357, 'grad_norm': 20.89728549129117, 'learning_rate': 7.564496387029531e-07, 'rewards/chosen': -1.3567368984222412, 'rewards/rejected': -1.914285659790039, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5575486421585083, 'logps/rejected': -0.765714168548584, 'logps/chosen': -0.5426946878433228, 'logits/rejected': -0.9974483251571655, 'logits/chosen': -1.0245858430862427, 'epoch': 0.4}\n",
            "{'loss': 1.4458, 'grad_norm': 14.284349268241145, 'learning_rate': 7.402210918896689e-07, 'rewards/chosen': -1.355165719985962, 'rewards/rejected': -2.093677282333374, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.7385115623474121, 'logps/rejected': -0.8374710083007812, 'logps/chosen': -0.5420663356781006, 'logits/rejected': -1.002201795578003, 'logits/chosen': -1.0203882455825806, 'epoch': 0.41}\n",
            "{'loss': 1.496, 'grad_norm': 21.006500802721117, 'learning_rate': 7.236565741578162e-07, 'rewards/chosen': -1.6456336975097656, 'rewards/rejected': -2.254960298538208, 'rewards/accuracies': 0.5874999761581421, 'rewards/margins': 0.6093264818191528, 'logps/rejected': -0.9019840359687805, 'logps/chosen': -0.6582534909248352, 'logits/rejected': -0.9267364740371704, 'logits/chosen': -0.9360872507095337, 'epoch': 0.42}\n",
            "{'loss': 1.4947, 'grad_norm': 17.34001688115439, 'learning_rate': 7.067792524832603e-07, 'rewards/chosen': -1.4712855815887451, 'rewards/rejected': -1.8979966640472412, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.42671123147010803, 'logps/rejected': -0.759198784828186, 'logps/chosen': -0.5885142087936401, 'logits/rejected': -0.9737011194229126, 'logits/chosen': -0.9733101725578308, 'epoch': 0.43}\n",
            "{'loss': 1.481, 'grad_norm': 18.93087179194272, 'learning_rate': 6.896127313264642e-07, 'rewards/chosen': -1.7645390033721924, 'rewards/rejected': -2.080124855041504, 'rewards/accuracies': 0.574999988079071, 'rewards/margins': 0.3155860900878906, 'logps/rejected': -0.8320499658584595, 'logps/chosen': -0.7058154940605164, 'logits/rejected': -1.0247756242752075, 'logits/chosen': -1.0645233392715454, 'epoch': 0.44}\n",
            "{'loss': 1.4931, 'grad_norm': 20.51304681116505, 'learning_rate': 6.721810196195174e-07, 'rewards/chosen': -1.6787519454956055, 'rewards/rejected': -2.114553451538086, 'rewards/accuracies': 0.6000000238418579, 'rewards/margins': 0.43580159544944763, 'logps/rejected': -0.8458214998245239, 'logps/chosen': -0.6715008020401001, 'logits/rejected': -0.9989116787910461, 'logits/chosen': -1.0714137554168701, 'epoch': 0.45}\n",
            "{'loss': 1.4357, 'grad_norm': 16.46301637400639, 'learning_rate': 6.545084971874736e-07, 'rewards/chosen': -1.9249680042266846, 'rewards/rejected': -2.397975444793701, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.47300752997398376, 'logps/rejected': -0.9591902494430542, 'logps/chosen': -0.7699872255325317, 'logits/rejected': -0.9812155961990356, 'logits/chosen': -1.0320379734039307, 'epoch': 0.46}\n",
            "{'loss': 1.4954, 'grad_norm': 21.088635750930834, 'learning_rate': 6.3661988065096e-07, 'rewards/chosen': -2.2241809368133545, 'rewards/rejected': -2.5182979106903076, 'rewards/accuracies': 0.625, 'rewards/margins': 0.2941167950630188, 'logps/rejected': -1.0073192119598389, 'logps/chosen': -0.8896724581718445, 'logits/rejected': -0.9874359965324402, 'logits/chosen': -1.0327280759811401, 'epoch': 0.47}\n",
            "{'loss': 1.557, 'grad_norm': 19.02142810522417, 'learning_rate': 6.185401888577487e-07, 'rewards/chosen': -1.9308984279632568, 'rewards/rejected': -1.9864448308944702, 'rewards/accuracies': 0.6499999761581421, 'rewards/margins': 0.0555465929210186, 'logps/rejected': -0.794577956199646, 'logps/chosen': -0.772359311580658, 'logits/rejected': -0.9660781621932983, 'logits/chosen': -0.9864276051521301, 'epoch': 0.48}\n",
            "{'loss': 1.4692, 'grad_norm': 17.867436451445265, 'learning_rate': 6.002947078916364e-07, 'rewards/chosen': -2.2047512531280518, 'rewards/rejected': -2.6978209018707275, 'rewards/accuracies': 0.625, 'rewards/margins': 0.49306946992874146, 'logps/rejected': -1.0791282653808594, 'logps/chosen': -0.8819006085395813, 'logits/rejected': -0.9232436418533325, 'logits/chosen': -1.0150645971298218, 'epoch': 0.49}\n",
            "{'loss': 1.4942, 'grad_norm': 19.261147315812543, 'learning_rate': 5.819089557075688e-07, 'rewards/chosen': -2.110212564468384, 'rewards/rejected': -2.4150078296661377, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.30479517579078674, 'logps/rejected': -0.9660031199455261, 'logps/chosen': -0.8440850377082825, 'logits/rejected': -0.9518272280693054, 'logits/chosen': -0.996496319770813, 'epoch': 0.5}\n",
            "{'loss': 1.4046, 'grad_norm': 18.32103206829717, 'learning_rate': 5.634086464424742e-07, 'rewards/chosen': -1.8911781311035156, 'rewards/rejected': -2.3508315086364746, 'rewards/accuracies': 0.6625000238418579, 'rewards/margins': 0.4596532881259918, 'logps/rejected': -0.9403325319290161, 'logps/chosen': -0.7564712762832642, 'logits/rejected': -1.006392002105713, 'logits/chosen': -1.0355093479156494, 'epoch': 0.51}\n",
            "{'loss': 1.4407, 'grad_norm': 13.624682866914808, 'learning_rate': 5.448196544517167e-07, 'rewards/chosen': -1.7677417993545532, 'rewards/rejected': -2.191777229309082, 'rewards/accuracies': 0.5375000238418579, 'rewards/margins': 0.4240352213382721, 'logps/rejected': -0.8767108917236328, 'logps/chosen': -0.7070968151092529, 'logits/rejected': -1.0080206394195557, 'logits/chosen': -1.0392498970031738, 'epoch': 0.52}\n",
            "{'loss': 1.467, 'grad_norm': 22.22843267157582, 'learning_rate': 5.26167978121472e-07, 'rewards/chosen': -2.1399290561676025, 'rewards/rejected': -3.0130460262298584, 'rewards/accuracies': 0.6625000238418579, 'rewards/margins': 0.8731171488761902, 'logps/rejected': -1.2052185535430908, 'logps/chosen': -0.8559715151786804, 'logits/rejected': -1.028188943862915, 'logits/chosen': -1.048105001449585, 'epoch': 0.53}\n",
            "{'loss': 1.4101, 'grad_norm': 21.7502246417763, 'learning_rate': 5.074797035076318e-07, 'rewards/chosen': -2.021688222885132, 'rewards/rejected': -2.4828155040740967, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.4611276090145111, 'logps/rejected': -0.9931262731552124, 'logps/chosen': -0.8086752891540527, 'logits/rejected': -1.0085898637771606, 'logits/chosen': -1.0093282461166382, 'epoch': 0.55}\n",
            "{'loss': 1.4384, 'grad_norm': 17.422008542886676, 'learning_rate': 4.887809678520975e-07, 'rewards/chosen': -1.8670291900634766, 'rewards/rejected': -2.731649875640869, 'rewards/accuracies': 0.7250000238418579, 'rewards/margins': 0.8646209836006165, 'logps/rejected': -1.0926599502563477, 'logps/chosen': -0.7468116879463196, 'logits/rejected': -0.9804226160049438, 'logits/chosen': -0.9935086369514465, 'epoch': 0.56}\n",
            "{'loss': 1.4835, 'grad_norm': 21.42359137838811, 'learning_rate': 4.700979230274829e-07, 'rewards/chosen': -2.19482684135437, 'rewards/rejected': -2.4993057250976562, 'rewards/accuracies': 0.6625000238418579, 'rewards/margins': 0.3044784963130951, 'logps/rejected': -0.9997223019599915, 'logps/chosen': -0.8779309391975403, 'logits/rejected': -1.0285662412643433, 'logits/chosen': -1.0343437194824219, 'epoch': 0.57}\n",
            "{'loss': 1.4746, 'grad_norm': 27.361650975575174, 'learning_rate': 4.514566989613559e-07, 'rewards/chosen': -2.058271646499634, 'rewards/rejected': -2.693838596343994, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.6355669498443604, 'logps/rejected': -1.0775353908538818, 'logps/chosen': -0.8233085870742798, 'logits/rejected': -1.030027151107788, 'logits/chosen': -1.022482991218567, 'epoch': 0.58}\n",
            "{'loss': 1.4355, 'grad_norm': 19.306011967200188, 'learning_rate': 4.328833670911724e-07, 'rewards/chosen': -2.444120168685913, 'rewards/rejected': -2.8809328079223633, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.43681272864341736, 'logps/rejected': -1.1523730754852295, 'logps/chosen': -0.977648138999939, 'logits/rejected': -0.841380774974823, 'logits/chosen': -0.8774306178092957, 'epoch': 0.59}\n",
            "{'loss': 1.4465, 'grad_norm': 18.830991297114817, 'learning_rate': 4.144039039010124e-07, 'rewards/chosen': -2.3203647136688232, 'rewards/rejected': -2.709719657897949, 'rewards/accuracies': 0.625, 'rewards/margins': 0.38935500383377075, 'logps/rejected': -1.0838878154754639, 'logps/chosen': -0.9281458854675293, 'logits/rejected': -1.034156084060669, 'logits/chosen': -1.0766229629516602, 'epoch': 0.6}\n",
            "{'loss': 1.4739, 'grad_norm': 20.938307778539542, 'learning_rate': 3.960441545911204e-07, 'rewards/chosen': -2.758822441101074, 'rewards/rejected': -2.8572330474853516, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.09841099381446838, 'logps/rejected': -1.1428934335708618, 'logps/chosen': -1.1035289764404297, 'logits/rejected': -0.9020398855209351, 'logits/chosen': -0.9466499090194702, 'epoch': 0.61}\n",
            "{'loss': 1.3943, 'grad_norm': 23.245653263780497, 'learning_rate': 3.778297969310529e-07, 'rewards/chosen': -2.283830165863037, 'rewards/rejected': -2.9133777618408203, 'rewards/accuracies': 0.6499999761581421, 'rewards/margins': 0.6295474767684937, 'logps/rejected': -1.1653510332107544, 'logps/chosen': -0.9135320782661438, 'logits/rejected': -0.9768760800361633, 'logits/chosen': -0.9969805479049683, 'epoch': 0.62}\n",
            "{'loss': 1.4446, 'grad_norm': 18.66753481518412, 'learning_rate': 3.5978630534699865e-07, 'rewards/chosen': -2.6337387561798096, 'rewards/rejected': -3.1272525787353516, 'rewards/accuracies': 0.6625000238418579, 'rewards/margins': 0.4935137629508972, 'logps/rejected': -1.2509009838104248, 'logps/chosen': -1.0534954071044922, 'logits/rejected': -1.0506360530853271, 'logits/chosen': -1.0683342218399048, 'epoch': 0.63}\n",
            "{'loss': 1.4048, 'grad_norm': 21.410873330760186, 'learning_rate': 3.4193891529348795e-07, 'rewards/chosen': -2.2726473808288574, 'rewards/rejected': -3.112048625946045, 'rewards/accuracies': 0.637499988079071, 'rewards/margins': 0.8394016027450562, 'logps/rejected': -1.2448195219039917, 'logps/chosen': -0.9090589284896851, 'logits/rejected': -0.8960458040237427, 'logits/chosen': -0.943785548210144, 'epoch': 0.64}\n",
            "{'loss': 1.4219, 'grad_norm': 16.99902973812307, 'learning_rate': 3.243125879593286e-07, 'rewards/chosen': -2.5598270893096924, 'rewards/rejected': -3.217184543609619, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.6573570370674133, 'logps/rejected': -1.286873698234558, 'logps/chosen': -1.0239307880401611, 'logits/rejected': -0.9785451889038086, 'logits/chosen': -1.001460075378418, 'epoch': 0.65}\n",
            "{'loss': 1.4695, 'grad_norm': 35.891043777341494, 'learning_rate': 3.069319753571269e-07, 'rewards/chosen': -2.619016170501709, 'rewards/rejected': -3.2953972816467285, 'rewards/accuracies': 0.6000000238418579, 'rewards/margins': 0.6763805747032166, 'logps/rejected': -1.3181588649749756, 'logps/chosen': -1.0476067066192627, 'logits/rejected': -1.024137258529663, 'logits/chosen': -0.9791940450668335, 'epoch': 0.66}\n",
            "{'loss': 1.469, 'grad_norm': 22.292608497489727, 'learning_rate': 2.898213858452173e-07, 'rewards/chosen': -2.593104124069214, 'rewards/rejected': -2.9195103645324707, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.3264063894748688, 'logps/rejected': -1.1678041219711304, 'logps/chosen': -1.0372415781021118, 'logits/rejected': -1.086306095123291, 'logits/chosen': -1.1296494007110596, 'epoch': 0.67}\n",
            "{'loss': 1.4399, 'grad_norm': 24.37127385244042, 'learning_rate': 2.730047501302266e-07, 'rewards/chosen': -2.4623827934265137, 'rewards/rejected': -3.1021530628204346, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6397703289985657, 'logps/rejected': -1.240861177444458, 'logps/chosen': -0.9849531054496765, 'logits/rejected': -1.019863486289978, 'logits/chosen': -1.0386595726013184, 'epoch': 0.68}\n",
            "{'loss': 1.3944, 'grad_norm': 28.3616490001571, 'learning_rate': 2.5650558779781635e-07, 'rewards/chosen': -2.544776439666748, 'rewards/rejected': -2.891955614089966, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.3471793532371521, 'logps/rejected': -1.1567823886871338, 'logps/chosen': -1.0179104804992676, 'logits/rejected': -0.9376241564750671, 'logits/chosen': -0.9190061688423157, 'epoch': 0.69}\n",
            "{'loss': 1.4179, 'grad_norm': 23.089017555469844, 'learning_rate': 2.403469744184154e-07, 'rewards/chosen': -2.6629960536956787, 'rewards/rejected': -3.638806104660034, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.9758095741271973, 'logps/rejected': -1.4555224180221558, 'logps/chosen': -1.065198540687561, 'logits/rejected': -0.9639641642570496, 'logits/chosen': -1.0184417963027954, 'epoch': 0.71}\n",
            "{'loss': 1.4107, 'grad_norm': 19.01160663909916, 'learning_rate': 2.2455150927394878e-07, 'rewards/chosen': -2.965078115463257, 'rewards/rejected': -3.4917080402374268, 'rewards/accuracies': 0.5874999761581421, 'rewards/margins': 0.5266298651695251, 'logps/rejected': -1.3966830968856812, 'logps/chosen': -1.1860311031341553, 'logits/rejected': -1.0168932676315308, 'logits/chosen': -0.9782741665840149, 'epoch': 0.72}\n",
            "{'loss': 1.4363, 'grad_norm': 24.023962985538827, 'learning_rate': 2.0914128375069722e-07, 'rewards/chosen': -2.5838780403137207, 'rewards/rejected': -3.448676586151123, 'rewards/accuracies': 0.675000011920929, 'rewards/margins': 0.8647986650466919, 'logps/rejected': -1.3794705867767334, 'logps/chosen': -1.0335513353347778, 'logits/rejected': -0.9652606248855591, 'logits/chosen': -0.9762552380561829, 'epoch': 0.73}\n",
            "{'loss': 1.3412, 'grad_norm': 24.338376358621, 'learning_rate': 1.9413785044249676e-07, 'rewards/chosen': -3.020373821258545, 'rewards/rejected': -3.4899115562438965, 'rewards/accuracies': 0.6000000238418579, 'rewards/margins': 0.4695381224155426, 'logps/rejected': -1.3959646224975586, 'logps/chosen': -1.2081493139266968, 'logits/rejected': -0.9039338827133179, 'logits/chosen': -0.9369735717773438, 'epoch': 0.74}\n",
            "{'loss': 1.3953, 'grad_norm': 25.7573169489427, 'learning_rate': 1.7956219300748792e-07, 'rewards/chosen': -2.622199773788452, 'rewards/rejected': -3.418231248855591, 'rewards/accuracies': 0.7124999761581421, 'rewards/margins': 0.7960314750671387, 'logps/rejected': -1.3672925233840942, 'logps/chosen': -1.048879861831665, 'logits/rejected': -1.0084810256958008, 'logits/chosen': -1.0008254051208496, 'epoch': 0.75}\n",
            "{'loss': 1.3954, 'grad_norm': 26.96889041092714, 'learning_rate': 1.6543469682057104e-07, 'rewards/chosen': -2.679978370666504, 'rewards/rejected': -3.1839847564697266, 'rewards/accuracies': 0.5874999761581421, 'rewards/margins': 0.5040060877799988, 'logps/rejected': -1.2735936641693115, 'logps/chosen': -1.0719913244247437, 'logits/rejected': -0.9677215814590454, 'logits/chosen': -1.0293567180633545, 'epoch': 0.76}\n",
            "{'loss': 1.3804, 'grad_norm': 21.661999756036945, 'learning_rate': 1.5177512046261666e-07, 'rewards/chosen': -3.2290101051330566, 'rewards/rejected': -4.164372444152832, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.9353626370429993, 'logps/rejected': -1.665748953819275, 'logps/chosen': -1.291603922843933, 'logits/rejected': -1.0177589654922485, 'logits/chosen': -0.9887057542800903, 'epoch': 0.77}\n",
            "{'loss': 1.3868, 'grad_norm': 21.098443523944745, 'learning_rate': 1.3860256808630427e-07, 'rewards/chosen': -3.009385347366333, 'rewards/rejected': -3.488234043121338, 'rewards/accuracies': 0.6000000238418579, 'rewards/margins': 0.4788486063480377, 'logps/rejected': -1.3952935934066772, 'logps/chosen': -1.203754186630249, 'logits/rejected': -0.9351035952568054, 'logits/chosen': -0.9724413752555847, 'epoch': 0.78}\n",
            "{'loss': 1.3739, 'grad_norm': 25.021776262301575, 'learning_rate': 1.2593546269723647e-07, 'rewards/chosen': -3.0019993782043457, 'rewards/rejected': -3.437342405319214, 'rewards/accuracies': 0.625, 'rewards/margins': 0.43534326553344727, 'logps/rejected': -1.3749370574951172, 'logps/chosen': -1.2007997035980225, 'logits/rejected': -1.0015369653701782, 'logits/chosen': -1.0209473371505737, 'epoch': 0.79}\n",
            "{'loss': 1.4606, 'grad_norm': 25.159927366829866, 'learning_rate': 1.1379152038770029e-07, 'rewards/chosen': -2.7455852031707764, 'rewards/rejected': -3.4570579528808594, 'rewards/accuracies': 0.675000011920929, 'rewards/margins': 0.7114725708961487, 'logps/rejected': -1.3828232288360596, 'logps/chosen': -1.0982341766357422, 'logits/rejected': -1.0289709568023682, 'logits/chosen': -1.0015761852264404, 'epoch': 0.8}\n",
            "{'loss': 1.3777, 'grad_norm': 20.84984615780728, 'learning_rate': 1.0218772555910954e-07, 'rewards/chosen': -3.073319911956787, 'rewards/rejected': -3.6821160316467285, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.6087957620620728, 'logps/rejected': -1.4728463888168335, 'logps/chosen': -1.2293280363082886, 'logits/rejected': -1.0346585512161255, 'logits/chosen': -1.087043046951294, 'epoch': 0.81}\n",
            "{'loss': 1.3372, 'grad_norm': 22.784469290812634, 'learning_rate': 9.114030716778432e-08, 'rewards/chosen': -3.019693613052368, 'rewards/rejected': -3.803895950317383, 'rewards/accuracies': 0.675000011920929, 'rewards/margins': 0.7842024564743042, 'logps/rejected': -1.5215582847595215, 'logps/chosen': -1.2078773975372314, 'logits/rejected': -1.0313512086868286, 'logits/chosen': -1.0404977798461914, 'epoch': 0.82}\n",
            "{'loss': 1.3633, 'grad_norm': 38.2013964652558, 'learning_rate': 8.066471602728803e-08, 'rewards/chosen': -3.110044479370117, 'rewards/rejected': -3.839865207672119, 'rewards/accuracies': 0.6499999761581421, 'rewards/margins': 0.7298205494880676, 'logps/rejected': -1.535946011543274, 'logps/chosen': -1.2440178394317627, 'logits/rejected': -1.0727328062057495, 'logits/chosen': -1.0984976291656494, 'epoch': 0.83}\n",
            "{'loss': 1.396, 'grad_norm': 21.29430884924605, 'learning_rate': 7.077560319906694e-08, 'rewards/chosen': -2.8958938121795654, 'rewards/rejected': -3.357959747314453, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.46206584572792053, 'logps/rejected': -1.3431838750839233, 'logps/chosen': -1.1583576202392578, 'logits/rejected': -0.817115306854248, 'logits/chosen': -0.9021504521369934, 'epoch': 0.84}\n",
            "{'loss': 1.3975, 'grad_norm': 18.542727738232298, 'learning_rate': 6.148679950161672e-08, 'rewards/chosen': -3.255145311355591, 'rewards/rejected': -3.9431610107421875, 'rewards/accuracies': 0.7250000238418579, 'rewards/margins': 0.6880159974098206, 'logps/rejected': -1.5772645473480225, 'logps/chosen': -1.3020581007003784, 'logits/rejected': -0.9371121525764465, 'logits/chosen': -0.9785951375961304, 'epoch': 0.85}\n",
            " 86%|█████████████████████████████████▍     | 400/467 [1:50:13<18:00, 16.12s/it][INFO|trainer.py:3719] 2024-06-14 01:00:15,118 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-06-14 01:00:15,119 >>   Num examples = 1961\n",
            "[INFO|trainer.py:3724] 2024-06-14 01:00:15,119 >>   Batch size = 4\n",
            "\n",
            "  0%|                                                    | 0/62 [00:00<?, ?it/s]\u001b[A\n",
            "  3%|█▍                                          | 2/62 [00:00<00:25,  2.37it/s]\u001b[A\n",
            "  5%|██▏                                         | 3/62 [00:01<00:38,  1.55it/s]\u001b[A\n",
            "  6%|██▊                                         | 4/62 [00:02<00:45,  1.27it/s]\u001b[A\n",
            "  8%|███▌                                        | 5/62 [00:03<00:45,  1.26it/s]\u001b[A\n",
            " 10%|████▎                                       | 6/62 [00:04<00:46,  1.19it/s]\u001b[A\n",
            " 11%|████▉                                       | 7/62 [00:05<00:45,  1.20it/s]\u001b[A\n",
            " 13%|█████▋                                      | 8/62 [00:06<00:46,  1.15it/s]\u001b[A\n",
            " 15%|██████▍                                     | 9/62 [00:07<00:42,  1.24it/s]\u001b[A\n",
            " 16%|██████▉                                    | 10/62 [00:08<00:45,  1.14it/s]\u001b[A\n",
            " 18%|███████▋                                   | 11/62 [00:10<01:03,  1.24s/it]\u001b[A\n",
            " 19%|████████▎                                  | 12/62 [00:11<01:07,  1.36s/it]\u001b[A\n",
            " 21%|█████████                                  | 13/62 [00:12<01:00,  1.24s/it]\u001b[A\n",
            " 23%|█████████▋                                 | 14/62 [00:14<01:05,  1.37s/it]\u001b[A\n",
            " 24%|██████████▍                                | 15/62 [00:15<00:56,  1.21s/it]\u001b[A\n",
            " 26%|███████████                                | 16/62 [00:17<01:06,  1.44s/it]\u001b[A\n",
            " 27%|███████████▊                               | 17/62 [00:18<00:58,  1.31s/it]\u001b[A\n",
            " 29%|████████████▍                              | 18/62 [00:19<00:52,  1.20s/it]\u001b[A\n",
            " 31%|█████████████▏                             | 19/62 [00:20<00:59,  1.39s/it]\u001b[A\n",
            " 32%|█████████████▊                             | 20/62 [00:22<00:54,  1.29s/it]\u001b[A\n",
            " 34%|██████████████▌                            | 21/62 [00:23<00:56,  1.38s/it]\u001b[A\n",
            " 35%|███████████████▎                           | 22/62 [00:24<00:54,  1.35s/it]\u001b[A\n",
            " 37%|███████████████▉                           | 23/62 [00:25<00:45,  1.17s/it]\u001b[A\n",
            " 39%|████████████████▋                          | 24/62 [00:26<00:42,  1.13s/it]\u001b[A\n",
            " 40%|█████████████████▎                         | 25/62 [00:27<00:38,  1.03s/it]\u001b[A\n",
            " 42%|██████████████████                         | 26/62 [00:28<00:34,  1.05it/s]\u001b[A\n",
            " 44%|██████████████████▋                        | 27/62 [00:29<00:35,  1.03s/it]\u001b[A\n",
            " 45%|███████████████████▍                       | 28/62 [00:30<00:37,  1.11s/it]\u001b[A\n",
            " 47%|████████████████████                       | 29/62 [00:32<00:38,  1.17s/it]\u001b[A\n",
            " 48%|████████████████████▊                      | 30/62 [00:34<00:46,  1.47s/it]\u001b[A\n",
            " 50%|█████████████████████▌                     | 31/62 [00:35<00:40,  1.30s/it]\u001b[A\n",
            " 52%|██████████████████████▏                    | 32/62 [00:36<00:38,  1.29s/it]\u001b[A\n",
            " 53%|██████████████████████▉                    | 33/62 [00:38<00:40,  1.40s/it]\u001b[A\n",
            " 55%|███████████████████████▌                   | 34/62 [00:39<00:36,  1.31s/it]\u001b[A\n",
            " 56%|████████████████████████▎                  | 35/62 [00:41<00:43,  1.63s/it]\u001b[A\n",
            " 58%|████████████████████████▉                  | 36/62 [00:42<00:35,  1.38s/it]\u001b[A\n",
            " 60%|█████████████████████████▋                 | 37/62 [00:43<00:30,  1.24s/it]\u001b[A\n",
            " 61%|██████████████████████████▎                | 38/62 [00:44<00:27,  1.17s/it]\u001b[A\n",
            " 63%|███████████████████████████                | 39/62 [00:45<00:25,  1.09s/it]\u001b[A\n",
            " 65%|███████████████████████████▋               | 40/62 [00:46<00:23,  1.05s/it]\u001b[A\n",
            " 66%|████████████████████████████▍              | 41/62 [00:47<00:23,  1.11s/it]\u001b[A\n",
            " 68%|█████████████████████████████▏             | 42/62 [00:48<00:20,  1.03s/it]\u001b[A\n",
            " 69%|█████████████████████████████▊             | 43/62 [00:49<00:19,  1.01s/it]\u001b[A\n",
            " 71%|██████████████████████████████▌            | 44/62 [00:50<00:17,  1.01it/s]\u001b[A\n",
            " 73%|███████████████████████████████▏           | 45/62 [00:51<00:17,  1.02s/it]\u001b[A\n",
            " 74%|███████████████████████████████▉           | 46/62 [00:52<00:16,  1.04s/it]\u001b[A\n",
            " 76%|████████████████████████████████▌          | 47/62 [00:54<00:18,  1.25s/it]\u001b[A\n",
            " 77%|█████████████████████████████████▎         | 48/62 [00:55<00:16,  1.21s/it]\u001b[A\n",
            " 79%|█████████████████████████████████▉         | 49/62 [00:56<00:15,  1.20s/it]\u001b[A\n",
            " 81%|██████████████████████████████████▋        | 50/62 [00:57<00:13,  1.13s/it]\u001b[A\n",
            " 82%|███████████████████████████████████▎       | 51/62 [00:58<00:11,  1.03s/it]\u001b[A\n",
            " 84%|████████████████████████████████████       | 52/62 [00:59<00:10,  1.06s/it]\u001b[A\n",
            " 85%|████████████████████████████████████▊      | 53/62 [01:01<00:11,  1.31s/it]\u001b[A\n",
            " 87%|█████████████████████████████████████▍     | 54/62 [01:02<00:11,  1.47s/it]\u001b[A\n",
            " 89%|██████████████████████████████████████▏    | 55/62 [01:03<00:09,  1.34s/it]\u001b[A\n",
            " 90%|██████████████████████████████████████▊    | 56/62 [01:04<00:07,  1.23s/it]\u001b[A\n",
            " 92%|███████████████████████████████████████▌   | 57/62 [01:06<00:06,  1.21s/it]\u001b[A\n",
            " 94%|████████████████████████████████████████▏  | 58/62 [01:07<00:04,  1.21s/it]\u001b[A\n",
            " 95%|████████████████████████████████████████▉  | 59/62 [01:08<00:03,  1.18s/it]\u001b[A\n",
            " 97%|█████████████████████████████████████████▌ | 60/62 [01:09<00:02,  1.17s/it]\u001b[A\n",
            " 98%|██████████████████████████████████████████▎| 61/62 [01:10<00:01,  1.08s/it]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 1.375516653060913, 'eval_runtime': 72.7978, 'eval_samples_per_second': 26.938, 'eval_steps_per_second': 0.852, 'eval_rewards/chosen': -2.9448394775390625, 'eval_rewards/rejected': -3.603750705718994, 'eval_rewards/accuracies': 0.6612903475761414, 'eval_rewards/margins': 0.6589111685752869, 'eval_logps/rejected': -1.4415003061294556, 'eval_logps/chosen': -1.1779358386993408, 'eval_logits/rejected': -1.1544642448425293, 'eval_logits/chosen': -1.1872848272323608, 'epoch': 0.85}\n",
            " 86%|█████████████████████████████████▍     | 400/467 [1:51:26<18:00, 16.12s/it]\n",
            "100%|███████████████████████████████████████████| 62/62 [01:11<00:00,  1.09s/it]\u001b[A\n",
            "                                                                                \u001b[A[2024-06-14 01:01:43,769] [WARNING] [stage3.py:1949:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
            "{'loss': 1.4197, 'grad_norm': 23.254725123506937, 'learning_rate': 5.2811296166831666e-08, 'rewards/chosen': -3.027691125869751, 'rewards/rejected': -3.6925175189971924, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.6648265719413757, 'logps/rejected': -1.4770066738128662, 'logps/chosen': -1.2110763788223267, 'logits/rejected': -0.8999165296554565, 'logits/chosen': -0.9737190008163452, 'epoch': 0.87}\n",
            "{'loss': 1.3848, 'grad_norm': 31.565832295867562, 'learning_rate': 4.4761226670592066e-08, 'rewards/chosen': -3.040944814682007, 'rewards/rejected': -3.7316603660583496, 'rewards/accuracies': 0.6499999761581421, 'rewards/margins': 0.6907154321670532, 'logps/rejected': -1.4926642179489136, 'logps/chosen': -1.2163779735565186, 'logits/rejected': -0.9287575483322144, 'logits/chosen': -0.9489310383796692, 'epoch': 0.88}\n",
            "{'loss': 1.3718, 'grad_norm': 22.344503337533215, 'learning_rate': 3.734784976300165e-08, 'rewards/chosen': -2.9978199005126953, 'rewards/rejected': -3.4492106437683105, 'rewards/accuracies': 0.6499999761581421, 'rewards/margins': 0.45139074325561523, 'logps/rejected': -1.3796842098236084, 'logps/chosen': -1.1991280317306519, 'logits/rejected': -0.913791298866272, 'logits/chosen': -0.92424076795578, 'epoch': 0.89}\n",
            "{'loss': 1.4021, 'grad_norm': 25.79666616123638, 'learning_rate': 3.058153372200695e-08, 'rewards/chosen': -3.255664348602295, 'rewards/rejected': -3.675905704498291, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.4202408194541931, 'logps/rejected': -1.4703620672225952, 'logps/chosen': -1.3022658824920654, 'logits/rejected': -0.9945958256721497, 'logits/chosen': -0.9958304166793823, 'epoch': 0.9}\n",
            "{'loss': 1.3444, 'grad_norm': 24.252773163543072, 'learning_rate': 2.4471741852423233e-08, 'rewards/chosen': -3.098480224609375, 'rewards/rejected': -3.9491209983825684, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8506406545639038, 'logps/rejected': -1.5796483755111694, 'logps/chosen': -1.2393920421600342, 'logits/rejected': -0.9239951968193054, 'logits/chosen': -0.9615448713302612, 'epoch': 0.91}\n",
            "{'loss': 1.3383, 'grad_norm': 25.840588676616843, 'learning_rate': 1.9027019250647036e-08, 'rewards/chosen': -2.9777793884277344, 'rewards/rejected': -3.513965606689453, 'rewards/accuracies': 0.6000000238418579, 'rewards/margins': 0.536186158657074, 'logps/rejected': -1.4055861234664917, 'logps/chosen': -1.19111168384552, 'logits/rejected': -0.895746111869812, 'logits/chosen': -0.9318382143974304, 'epoch': 0.92}\n",
            "{'loss': 1.3959, 'grad_norm': 24.38677858349731, 'learning_rate': 1.4254980853566246e-08, 'rewards/chosen': -2.9474682807922363, 'rewards/rejected': -3.747727870941162, 'rewards/accuracies': 0.675000011920929, 'rewards/margins': 0.8002597093582153, 'logps/rejected': -1.4990911483764648, 'logps/chosen': -1.1789872646331787, 'logits/rejected': -0.9452959299087524, 'logits/chosen': -0.9664777517318726, 'epoch': 0.93}\n",
            "{'loss': 1.3646, 'grad_norm': 20.846346077184034, 'learning_rate': 1.016230078838226e-08, 'rewards/chosen': -3.0125012397766113, 'rewards/rejected': -3.535521984100342, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5230205059051514, 'logps/rejected': -1.4142088890075684, 'logps/chosen': -1.205000638961792, 'logits/rejected': -0.9755558967590332, 'logits/chosen': -0.9898012280464172, 'epoch': 0.94}\n",
            "{'loss': 1.3825, 'grad_norm': 25.087392613196315, 'learning_rate': 6.754703038239329e-09, 'rewards/chosen': -2.887716770172119, 'rewards/rejected': -3.263856887817383, 'rewards/accuracies': 0.6625000238418579, 'rewards/margins': 0.3761400282382965, 'logps/rejected': -1.3055427074432373, 'logps/chosen': -1.1550867557525635, 'logits/rejected': -0.9080606698989868, 'logits/chosen': -0.9521238207817078, 'epoch': 0.95}\n",
            "{'loss': 1.3408, 'grad_norm': 24.725896653899223, 'learning_rate': 4.036953436716895e-09, 'rewards/chosen': -3.194871425628662, 'rewards/rejected': -3.8272438049316406, 'rewards/accuracies': 0.6625000238418579, 'rewards/margins': 0.6323727369308472, 'logps/rejected': -1.5308974981307983, 'logps/chosen': -1.2779486179351807, 'logits/rejected': -0.9464728236198425, 'logits/chosen': -0.9332659840583801, 'epoch': 0.96}\n",
            "{'loss': 1.4589, 'grad_norm': 28.975077371611935, 'learning_rate': 2.0128530023804656e-09, 'rewards/chosen': -3.3643393516540527, 'rewards/rejected': -3.5827383995056152, 'rewards/accuracies': 0.5249999761581421, 'rewards/margins': 0.21839912235736847, 'logps/rejected': -1.4330954551696777, 'logps/chosen': -1.345735788345337, 'logits/rejected': -0.9457721710205078, 'logits/chosen': -0.9732203483581543, 'epoch': 0.97}\n",
            "{'loss': 1.3759, 'grad_norm': 26.785266092328502, 'learning_rate': 6.852326227130833e-10, 'rewards/chosen': -3.596348524093628, 'rewards/rejected': -4.503045558929443, 'rewards/accuracies': 0.6499999761581421, 'rewards/margins': 0.906697154045105, 'logps/rejected': -1.8012183904647827, 'logps/chosen': -1.4385395050048828, 'logits/rejected': -0.9895550608634949, 'logits/chosen': -0.9476199150085449, 'epoch': 0.98}\n",
            "{'loss': 1.3506, 'grad_norm': 25.28595563871724, 'learning_rate': 5.594909486328348e-11, 'rewards/chosen': -2.8763339519500732, 'rewards/rejected': -3.5513739585876465, 'rewards/accuracies': 0.6625000238418579, 'rewards/margins': 0.6750401258468628, 'logps/rejected': -1.4205496311187744, 'logps/chosen': -1.1505335569381714, 'logits/rejected': -0.9505317807197571, 'logits/chosen': -1.0385969877243042, 'epoch': 0.99}\n",
            "100%|███████████████████████████████████████| 467/467 [2:10:07<00:00, 16.36s/it][INFO|trainer.py:2329] 2024-06-14 01:20:08,328 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 7808.6202, 'train_samples_per_second': 7.668, 'train_steps_per_second': 0.06, 'train_loss': 1.4758259897824273, 'epoch': 1.0}\n",
            "100%|███████████████████████████████████████| 467/467 [2:10:07<00:00, 16.72s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =     0.9981\n",
            "  total_flos               =        0GF\n",
            "  train_loss               =     1.4758\n",
            "  train_runtime            = 2:10:08.62\n",
            "  train_samples            =      59876\n",
            "  train_samples_per_second =      7.668\n",
            "  train_steps_per_second   =       0.06\n",
            "2024-06-14 01:20:08 - INFO - __main__ - *** Training complete ***\n",
            "2024-06-14 01:20:08 - INFO - __main__ - *** Save model ***\n",
            "[INFO|trainer.py:3410] 2024-06-14 01:20:19,641 >> Saving model checkpoint to outputs/llama-3-8b-instruct-simpo\n",
            "[INFO|configuration_utils.py:472] 2024-06-14 01:20:19,649 >> Configuration saved in outputs/llama-3-8b-instruct-simpo/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-06-14 01:20:19,653 >> Configuration saved in outputs/llama-3-8b-instruct-simpo/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-06-14 01:21:07,258 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/llama-3-8b-instruct-simpo/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-06-14 01:21:07,266 >> tokenizer config file saved in outputs/llama-3-8b-instruct-simpo/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-06-14 01:21:07,271 >> Special tokens file saved in outputs/llama-3-8b-instruct-simpo/special_tokens_map.json\n",
            "2024-06-14 01:21:09 - INFO - __main__ - Model saved to outputs/llama-3-8b-instruct-simpo\n",
            "[INFO|modelcard.py:450] 2024-06-14 01:21:09,104 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'dataset': {'name': 'princeton-nlp/llama3-ultrafeedback', 'type': 'princeton-nlp/llama3-ultrafeedback'}}\n",
            "[INFO|configuration_utils.py:472] 2024-06-14 01:21:09,117 >> Configuration saved in outputs/llama-3-8b-instruct-simpo/config.json\n",
            "2024-06-14 01:21:09 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3719] 2024-06-14 01:21:09,119 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-06-14 01:21:09,119 >>   Num examples = 1961\n",
            "[INFO|trainer.py:3724] 2024-06-14 01:21:09,120 >>   Batch size = 4\n",
            "100%|███████████████████████████████████████████| 62/62 [01:10<00:00,  1.14s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     0.9981\n",
            "  eval_logits/chosen      =    -1.1943\n",
            "  eval_logits/rejected    =    -1.1612\n",
            "  eval_logps/chosen       =     -1.206\n",
            "  eval_logps/rejected     =    -1.4741\n",
            "  eval_loss               =     1.3735\n",
            "  eval_rewards/accuracies =     0.6653\n",
            "  eval_rewards/chosen     =    -3.0149\n",
            "  eval_rewards/margins    =     0.6704\n",
            "  eval_rewards/rejected   =    -3.6852\n",
            "  eval_runtime            = 0:01:11.49\n",
            "  eval_samples            =       1961\n",
            "  eval_samples_per_second =     27.428\n",
            "  eval_steps_per_second   =      0.867\n",
            "2024-06-14 01:22:20 - INFO - __main__ - *** Training complete! ***\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.093 MB of 0.093 MB uploaded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/logits/chosen █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/logits/rejected █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/logps/chosen █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/logps/rejected █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/rewards/accuracies ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/rewards/chosen █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/rewards/margins ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    eval/rewards/rejected █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/runtime █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/samples_per_second ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    eval/steps_per_second ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm ▂▂▁▄▁▁▃▁▂▃▂▂▂▃▂▂▂▃▄▃▃▄▃▅▄▃▇▅▄▅▅▄▄█▄▄▄▄▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate ▁▂▄▆███████▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/logits/chosen ▁▆▆▆██▆▆▅▆█▅▆▆▅▆▅▆▃▆▅▄▅▅▇▃▆▄▅▇▄▆▃▂▆█▆▆▇▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/logits/rejected ▃▅▄▆█▇▄▄▂▅▆▂▄▅▅▄▃▄▃▄▅▂▄▂▆▂▂▃▄▆▄▅▂▁▆▆▅▄▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/logps/chosen ████▇█▇█▇▇▇▇▇▆▆▅▆▆▅▄▄▄▅▄▂▃▃▃▂▁▂▂▁▁▁▂▁▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/logps/rejected ██████▇▇▇▇▇▆▇▆▆▅▅▅▅▅▄▃▄▄▃▃▂▃▂▂▃▂▂▁▂▂▁▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss █████▇▇▆▆▆▆▆▆▅▅▄▄▅▅▇▅▄▄▄▄▄▄▄▃▁▂▂▂▂▃▂▁▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/rewards/accuracies ▅▁▃▄▂▇▁▂▄▇▂▁▃▄▃▆▇▇▅▆▃▆█▅▃▆▅▅▇▅▄▅▅▆▇▆▅▅▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/rewards/chosen ████▇█▇█▇▇▇▇▇▆▆▅▆▆▅▄▄▄▅▄▂▃▃▃▂▁▂▂▁▁▁▂▁▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/rewards/margins ▁▁▁▁▁▂▂▂▂▃▁▄▂▅▂▅▆▄▄▁▃▇▇▆▂▅▆▆█▄▅▄▅▆▆▄▇▅▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/rewards/rejected ██████▇▇▇▇▇▆▇▆▆▅▅▅▅▅▄▃▄▄▃▃▂▃▂▂▃▂▂▁▂▂▁▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       eval/logits/chosen -1.19434\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/logits/rejected -1.16119\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/logps/chosen -1.20595\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/logps/rejected -1.4741\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/loss 1.37345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/rewards/accuracies 0.66532\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/rewards/chosen -3.01488\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     eval/rewards/margins 0.67037\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    eval/rewards/rejected -3.68525\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/runtime 71.4975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  eval/samples_per_second 27.428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    eval/steps_per_second 0.867\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               total_flos 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch 0.99813\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step 467\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm 25.28596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/logits/chosen -1.0386\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/logits/rejected -0.95053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/logps/chosen -1.15053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/logps/rejected -1.42055\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 1.3506\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/rewards/accuracies 0.6625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/rewards/chosen -2.87633\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/rewards/margins 0.67504\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/rewards/rejected -3.55137\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 1.47583\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_runtime 7808.6202\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_samples_per_second 7.668\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_steps_per_second 0.06\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mllama-3-8b-instruct-simpo\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/raylee0/huggingface/runs/8ewknmag\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/raylee0/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240613_231000-8ewknmag/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!cd SimPOW && ACCELERATE_LOG_LEVEL=info accelerate launch --config_file accelerate_configs/deepspeed_zero3.yaml scripts/run_simpo.py training_configs/llama-3-8b-instruct-simpo.yaml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1709c0dae6754fb7b5b3d36ad8656e60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ab95b523a0f4472af91e13f72b3c368": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30c0bc0798564e2fb336f08efc49f8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d31744072ca479792f8e6fc15697bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8eb2c633455440018ebf35f1cb28f785",
            "placeholder": "​",
            "style": "IPY_MODEL_eedb241c467d438a829193e1d11c3c4e",
            "value": ""
          }
        },
        "43b3553fc00a4090a8eeae07abc21300": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4d8fa997a6bd4f70a85bbd1fcc24a14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4fab0e2c93a47919a4f5f6faa69f34c",
            "placeholder": "​",
            "style": "IPY_MODEL_758f405ed9844b4fa0a700ea23d4ac56",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "5634a0ba7df043b28414f2485143c150": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6d1abc28ed00413cba61edffa52ca51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d8fa997a6bd4f70a85bbd1fcc24a14c",
              "IPY_MODEL_3d31744072ca479792f8e6fc15697bac",
              "IPY_MODEL_db58c927b0564b4eba2084c8c6419cf0",
              "IPY_MODEL_b46ce49263864ea69abfc64ea490554b",
              "IPY_MODEL_c60b117cdbc34017b5a87d99ab2b5b0c"
            ],
            "layout": "IPY_MODEL_5634a0ba7df043b28414f2485143c150"
          }
        },
        "758f405ed9844b4fa0a700ea23d4ac56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "811234b35e424e1fbfb413edd8071ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb2c633455440018ebf35f1cb28f785": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af57d4292200416b9f35a1819ebb99e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b46ce49263864ea69abfc64ea490554b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_1709c0dae6754fb7b5b3d36ad8656e60",
            "style": "IPY_MODEL_43b3553fc00a4090a8eeae07abc21300",
            "tooltip": ""
          }
        },
        "c4fab0e2c93a47919a4f5f6faa69f34c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60b117cdbc34017b5a87d99ab2b5b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30c0bc0798564e2fb336f08efc49f8d7",
            "placeholder": "​",
            "style": "IPY_MODEL_2ab95b523a0f4472af91e13f72b3c368",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "db58c927b0564b4eba2084c8c6419cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_811234b35e424e1fbfb413edd8071ce2",
            "style": "IPY_MODEL_af57d4292200416b9f35a1819ebb99e1",
            "value": true
          }
        },
        "eedb241c467d438a829193e1d11c3c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
